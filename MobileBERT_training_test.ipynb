{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from utils.formats import load_hdf\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from datasets import Dataset\n",
    "from transformers import PreTrainedModel, AutoModel, AutoTokenizer, MobileBertForSequenceClassification \n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers.configuration_utils import PretrainedConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Huggingface model - [MobileBERT](https://huggingface.co/docs/transformers/model_doc/mobilebert#transformers.MobileBertForSequenceClassification)\n",
    "\n",
    "* Input Embedding Dimensionality cannot be too big. \n",
    "* Standard Flavours of BERT-based transformer models have input dim of 768. PPMi + Retrofitting takes too long to produce input embedding vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer Vocab Size: 2016\n",
      "Embedding Dimensionality: 128\n",
      "Vocab:\n",
      "{'##6': 75, 'buddy': 1792, '##ow': 101, 'bo': 557, '##ass': 624, 'des': 1039, 'rain': 1911, 'yep': 1340, '##self': 535, 'eas': 1644, 'best': 479, 'romantic': 1691, 'once': 1489, '##ingfacewithsun': 1648, 'you': 80, 'cheat': 1015, 'says': 1683, 'wrong': 583, 'last': 867, 'type': 1093, 'problems': 1771, 'dist': 1104, 'where': 302, 'rom': 1410, 'dare': 1813, '##ps': 1066, 'might': 1417, 'problem': 684, '##ge': 277, '##ware': 1433, 'absol': 1825, 'li': 127, 'smilingcatfacewithhearteyes': 1684, 'fem': 1705, 'car': 1181, 'mil': 1797, 'sc': 653, 'gi': 1046, '##severingface': 1712, 'didn': 526, '##ally': 417, '##ck': 199, 'grinn': 363, 'proba': 1436, 'dance': 1267, '##winkingface': 1678, 'ref': 1985, 'sub': 1699, 'pain': 948, 'true': 881, '##grinningfacewithsweat': 1304, 'laugh': 682, 'new': 593, '##ant': 568, '##at': 82, 'girls': 1227, 'thn': 1354, 'lie': 1299, 'a': 16, 'got': 478, 'makes': 1025, '##and': 303, '##rown': 872, 'favourite': 1386, 'brokenheart': 1947, 'engine': 1512, 'r': 33, 'dinner': 1036, 'intern': 1709, 'sat': 1818, '##gr': 931, 'thnx': 1925, 'disappointedface': 788, 'bed': 909, '##ult': 1347, 'rude': 523, '##bl': 693, 'is': 118, '##ong': 260, 'grinningfacewithsweat': 1010, 'ag': 425, '##ink': 227, 'met': 1599, '##with': 117, 'used': 1460, 'forg': 842, 'part': 748, 'yea': 253, '##ror': 1892, '##eep': 350, '8': 14, '##4': 73, 'con': 386, '##not': 1522, 'watch': 516, '##steamfrom': 1013, 'em': 820, 'ha': 105, 'an': 106, '##f': 60, 'ab': 192, 'give': 423, 'facewithtearsofjoyfacewithtearsofjoyfacewithtearsofjoy': 644, 'getting': 770, 'looks': 1478, 'how': 169, '##ither': 985, 'trash': 2007, '##ache': 1706, 'giving': 1708, '##astic': 1437, '##ark': 1373, 'tw': 907, 'microsof': 1498, 'wo': 1579, 'crush': 1401, 'sleeping': 1538, '##mes': 1059, 'watching': 1161, '##ese': 886, 'going': 382, 'saw': 1494, 'thumbs': 1124, 'bea': 751, 'ba': 311, 'wan': 185, '##ip': 608, 'baby': 595, 'kind': 630, 'reg': 1877, 'm': 28, 'should': 439, 'relievedface': 1335, 'weekend': 1482, '##log': 1459, '##use': 1735, 'vide': 951, 'calling': 1723, 'hell': 501, 'ear': 911, 'colle': 1196, 'nt': 1332, '##ist': 483, '##ince': 1215, 'come': 419, 'her': 569, 'us': 528, 'ugly': 1367, 'kill': 1692, 'food': 993, 'learn': 1250, 'mind': 685, 'foot': 1763, '##pensiveface': 1914, 'el': 729, '##sun': 1597, '##ind': 301, '##ore': 406, 'mo': 207, 'ahead': 1956, '##act': 703, 'music': 1055, '##ens': 930, 'worry': 1404, 'propose': 1988, 'ah': 1021, '##son': 1047, 'hav': 1865, 'bet': 481, 'probably': 1513, 'beaut': 914, 'wa': 507, 'down': 894, '##rimac': 1610, 'compliment': 1955, '##b': 62, '##ete': 1288, 'quite': 1564, 'eating': 1889, 'worst': 1649, '##ther': 384, 'pass': 1517, '##ness': 1474, 'pictures': 1638, '##ri': 145, 'myself': 969, 'man': 426, 'asking': 686, 'month': 1479, 'under': 475, 'life': 424, '##hip': 1589, '##if': 430, 'veg': 1951, 'depress': 1054, 'my': 126, '##rible': 1944, 'relation': 1064, '5': 11, 'room': 1388, 'appre': 1905, 'intelligence': 1313, '##steam': 994, 'glad': 827, 'sleepy': 1743, '##times': 1155, 'turn': 1758, 'cuz': 1759, '##ww': 830, '##ical': 1173, '##ship': 1069, 'me': 99, '##am': 230, 'bad': 315, '##ket': 940, 'only': 415, '##iment': 1867, '##orn': 1165, 'clear': 1174, '##ital': 2003, 'wearyface': 1632, 'pro': 347, '##fer': 1157, '##outh': 979, 'need': 385, '##set': 1108, '##ie': 279, '##grinn': 447, 'hw': 1932, 'differe': 1189, '##ash': 1270, 'thought': 651, '##ith': 100, 'sleep': 463, 'hurting': 1810, 'anyway': 1345, '##ro': 165, 'at': 295, 'artificial': 1551, '##ive': 317, 'goo': 1056, '##hands': 1625, '##conf': 1675, 'who': 259, 'u': 36, 'catch': 1972, 'spec': 1083, '##unch': 1228, '##the': 1895, '##mil': 205, '##ange': 898, '##light': 2000, 'woman': 1773, 'dr': 1043, 'thou': 484, '##pend': 1323, 'join': 1509, '##pped': 1742, '##quintingfacewithtongue': 1725, 'pu': 1750, 'may': 554, '##blowingak': 836, '##ister': 1539, 'plea': 326, 'om': 1934, 'things': 704, '##fect': 1096, 'cold': 1562, '##augh': 638, '##ience': 1156, 'su': 233, 'fa': 372, 'spe': 967, '##en': 97, '##winkingfacewithtongue': 1718, 'interest': 710, 'serious': 665, '##reak': 719, 'col': 1526, 'umm': 1613, 'plan': 843, 'the': 107, '##d': 56, '##ute': 722, 'yaar': 1194, 's': 34, 'faceblowingakiss': 1783, 'asked': 831, 'frnd': 1466, 'real': 616, '##ition': 1824, 'j': 25, '##m': 50, '##ar': 170, 'let': 420, 'hahaha': 659, 'cu': 1308, '##eart': 271, 'language': 998, 'them': 625, '##cl': 1983, 'many': 695, 'z': 41, 'idea': 1123, 't': 35, 'opin': 1852, 'loudlycryingface': 652, '##ingfacewithhearteyes': 637, 'means': 656, 'women': 1958, 'angry': 445, 'he': 314, '##ird': 1274, 'drink': 1314, 'gott': 1804, 'even': 482, 'app': 760, 'this': 236, '##ryingface': 235, '##om': 476, '##ving': 531, 'yr': 1766, 'exist': 2006, 'hurt': 560, 'soo': 1225, 'hor': 1387, 'intell': 920, '##een': 1268, '##red': 539, 'face': 224, \"'\": 5, '##pe': 366, 'sent': 875, 'message': 971, 'least': 1592, '##lieve': 586, 'english': 734, '##usedface': 834, 'reading': 1754, 'second': 1850, '##disappointedface': 702, 'search': 1604, 'must': 1106, 'qu': 395, '##rong': 551, 'belong': 1887, 'plz': 857, 'chatting': 991, 'hot': 1201, 'devel': 1722, '12': 1747, 'mom': 1403, 'emot': 1689, '##day': 349, '##lc': 524, 'still': 594, '##all': 1281, '##ice': 304, '##v': 59, '##ons': 861, 'nobody': 1242, 'ey': 1352, 'pizza': 1514, 'mood': 854, '##on': 88, 'ai': 774, 'smilingfacewithsmilingeyes': 1187, '##ress': 502, 'dat': 1762, '##catface': 460, '##cat': 433, 'n': 29, '##bo': 750, 'hu': 786, '##angu': 859, '##facean': 1277, '##haha': 1290, 'war': 1875, 'always': 474, 'boy': 572, 'break': 778, '##ise': 901, 'fin': 1089, 'telling': 1336, '##re': 84, 'class': 1361, 'pre': 495, 'fact': 1786, 'on': 139, '##ke': 122, 'irritating': 798, 'dp': 1278, 'wish': 711, '##ats': 1418, 'sorry': 330, 'yup': 809, 'add': 992, '##faceangry': 1728, '##ire': 1353, 'confused': 1886, 'boys': 1904, 'ton': 1502, 'self': 1029, 'fo': 1099, '##ilm': 1357, 'usually': 2009, 'lets': 1001, 'wtf': 1812, 'mon': 632, '##j': 64, 'de': 396, '##ible': 1263, 'plans': 1920, 'program': 1168, '##asses': 1602, 'wat': 398, 'facewithtearsofjoy': 265, 'ignore': 1572, 'over': 730, 'grinningfacewith': 691, '##ud': 264, 'ca': 310, 'house': 1309, '##vers': 983, 'busy': 819, '##sw': 392, '##tiredface': 1954, '##er': 90, '##ld': 190, 'tra': 1202, '##ag': 587, '##oot': 1329, '[MASK]': 4, 'nd': 1280, 'keep': 813, 'didnt': 1359, '##per': 561, 'stupid': 375, 'later': 1038, '##smilingfacewithhearteyes': 988, '##cryingface': 254, 'har': 775, '##ouble': 1546, 'inter': 1172, 'bored': 1141, 'came': 1658, 'star': 1769, '##skint': 1775, '##lock': 964, '##gn': 936, '##skintone': 1856, '##q': 66, 'eyes': 1690, 'every': 418, 'missing': 1360, '##reat': 413, '##rit': 465, 'text': 986, 'tv': 1444, 'before': 1079, '##eak': 744, '##ey': 191, 'when': 367, 'human': 692, 'again': 510, '##pression': 1272, 'feeling': 506, 'hurts': 1235, '##its': 1491, 'enough': 919, 'imag': 1807, 'se': 373, 'of': 149, 'depends': 1565, '##ular': 1868, '##ings': 401, 'comput': 1376, 'disturb': 1364, 'whom': 1381, 'watched': 1530, 'sm': 737, '##ried': 789, 'prett': 733, '##gether': 1508, 'ter': 1713, 'ro': 563, '##vel': 927, 'absolutely': 1928, '##ling': 1026, 'smilingface': 1948, '##est': 240, 'good': 183, '##cing': 1822, 'week': 850, '##withtearsofjoy': 157, '##poutingface': 736, '##facewithtearsofjoyfacewithtearsofjoy': 357, '##ci': 1220, '##ite': 543, 'fool': 1443, 'micro': 1470, 'weird': 1458, '##9': 74, 'facewithtearsofjoyfacewithtearsofjoy': 654, '##press': 634, 'leaving': 1829, '##ightly': 826, '##lie': 565, 'wanted': 1325, '##z': 67, '##ti': 848, 'these': 1027, 'headache': 1790, 'missed': 1122, 'late': 1190, '##un': 444, 'list': 732, 'breakfast': 1991, 'water': 1650, 'stup': 374, '##rollingeyes': 1853, 'ex': 306, 'tur': 1261, 'wt': 915, '##ch': 143, 'point': 1169, 'your': 144, 'its': 369, 'lear': 954, 'want': 215, '1': 7, '##ee': 136, '##ingcatface': 485, '##apo': 981, '##ture': 699, 'yo': 1567, 'exact': 1110, 'suffering': 1793, 'moment': 1826, '##h': 45, 'future': 1421, 'rel': 796, '##lieved': 863, '##itely': 1338, 'prefer': 1787, 'proper': 1969, 'lik': 1231, 'joking': 1496, 'ra': 1751, '##iful': 1008, '##umbs': 879, 'ann': 662, 'nat': 1631, '##ial': 712, 'repl': 1560, 'thumbsup': 1485, 'mother': 1917, 'res': 869, 'soon': 949, '##ich': 378, 'math': 1554, '##one': 223, 'prom': 1664, 'coup': 1919, '##ath': 636, '##li': 1269, 'though': 845, 'idk': 1882, 'luck': 913, 'cheated': 1468, '##unamusedface': 1449, 'damn': 974, 'forgot': 1531, 'school': 1179, 'blue': 1883, '##ight': 244, '##ding': 641, 'choice': 1619, '##ame': 241, '##o': 49, 'tru': 1075, 'oo': 1455, '##other': 952, 'pa': 1764, '##ys': 1980, '##inkingface': 694, 'photo': 679, '##up': 266, 'die': 1794, '##ors': 1524, 'shut': 849, 'comp': 852, 'winkingface': 1799, 'experience': 1888, 'ass': 1284, 'cric': 1749, 'question': 530, 'ever': 332, '##ust': 178, 'di': 1118, '##ber': 804, 'gud': 1576, '##ue': 412, '##eal': 923, 'um': 1752, '##loudlycryingfaceloudlycryingface': 493, 'making': 1133, 'book': 1045, 'att': 1199, 'sk': 1273, 'engl': 715, 'gr': 966, '##au': 237, 'beautiful': 1019, '##w': 63, 'know': 153, 'number': 606, 'pick': 1493, 'bl': 726, 'could': 668, '##ning': 690, '##ering': 1254, '2': 8, 'old': 868, 'facewithsteamfromnose': 1467, 'sarc': 1519, 'lovely': 1901, 'cha': 1472, 'secret': 1246, 'babe': 1563, '##ya': 1913, 'gonna': 795, 'fuck': 379, '##gl': 590, '##are': 639, 'fe': 735, 'dnt': 1316, 'song': 635, 'far': 1342, '##hearteyes': 549, 'sha': 648, '##ract': 1673, 'country': 1441, '##ingfacewith': 222, 'from': 364, 'smilingfacewithhearteyes': 932, 'ho': 745, '##mber': 960, 'cant': 888, '##ion': 212, 'catfacewithtearsofjoy': 1495, 'smil': 462, 'useless': 1906, 'possible': 1686, '##earsof': 155, '##tt': 442, 'won': 610, 'say': 262, 'actually': 670, 'en': 353, '##ild': 1490, 'god': 1216, '##ble': 404, 'sun': 1866, '##gram': 1140, '##ool': 342, 'had': 558, 'char': 1435, 'pur': 1239, 'cool': 414, 'movies': 1062, 'deep': 1930, 'job': 757, 'guys': 1414, 'better': 553, 'into': 1529, '##ack': 1213, 'playing': 1687, 'one': 248, 'ph': 434, 'party': 1385, '##outingcatface': 1844, 'appreci': 1929, 'having': 1004, '##by': 496, 'du': 1630, 'tel': 1990, '##steamfromnose': 1035, 'upset': 1185, 'mak': 688, 'kid': 783, '##ically': 1805, 'team': 1863, 'lo': 138, 'sec': 892, '##0': 72, 'count': 1154, '##idd': 1570, '##ide': 545, '##ick': 706, 'personal': 1770, '##ustr': 1986, 'sending': 1828, '##ound': 582, '##ush': 865, 'am': 135, 'redheart': 1586, '##irk': 1465, 'non': 1733, 'which': 400, 'creat': 990, '##g': 44, 'mobile': 1434, '##lievedface': 877, 'state': 1741, '##cent': 1464, 'send': 333, 'bu': 837, 'there': 381, '##rite': 880, 'important': 1760, 'answer': 609, 'isn': 1265, 'haven': 1255, 'our': 883, 'first': 614, '##gg': 740, 'able': 1616, '##more': 1136, '##oint': 383, '##ation': 399, '##ount': 1679, 'doing': 490, 'uh': 1203, '##wn': 956, 'can': 161, '##oooo': 1127, 'suff': 1583, '##av': 968, 'looking': 1063, 'much': 362, 'mat': 856, 'already': 725, 'o': 30, 'cause': 950, 'tht': 1669, 'yeah': 287, '##from': 1003, '##ift': 1271, 'ug': 1327, 'annoy': 667, '##inn': 238, '##thing': 195, 'now': 218, 'co': 209, 'anyone': 1121, '##ari': 1204, 'da': 1351, '##ft': 769, 'coff': 1492, '##rowningfacewithopenmouth': 1973, 'half': 1580, '##ries': 1943, '##pose': 1321, 'free': 959, 'knew': 1319, '##2': 69, '##quint': 621, '##ol': 878, 'gone': 1676, '##ut': 134, 'seem': 970, 'hehe': 1061, 'that': 131, '##ra': 376, '##ady': 669, '##grinningface': 1076, '##ause': 305, '##7': 71, '##za': 1423, 'nor': 1424, 'inf': 1488, '##ightlyfrowningface': 1707, '##man': 1297, 'eng': 1337, 'ur': 298, 'boyfriend': 882, '##right': 1131, 'she': 540, 'were': 601, 'it': 108, '##reen': 1780, '##ming': 1716, 'wh': 85, '##ense': 764, 'single': 1087, 'those': 1206, '##gs': 1334, '##fac': 1229, '##ific': 1074, 'str': 1548, 'fee': 289, 'sor': 323, 'person': 575, 'out': 432, 'him': 821, 'does': 552, '##facewith': 980, '##ouse': 1205, 'address': 1726, '##owing': 767, '##sc': 1779, 'cou': 818, '##pl': 449, '##ather': 1082, '##loudlycryingfaceloudlycryingfaceloudlycryingfaceloudlycryingface': 1221, '##ile': 756, 'honest': 1484, 'sure': 457, 'boring': 862, 'friend': 293, '##hh': 468, '##ating': 584, '##line': 1163, 'del': 1528, 'lol': 409, 'loud': 1553, 'happen': 533, 'ser': 555, 'hmm': 499, '##00': 1646, '##ored': 973, 'enjoying': 1249, 'hop': 765, '##eary': 917, '##ign': 1374, 'langu': 874, 'knw': 1324, 'other': 646, 'imp': 1208, '##di': 464, 'marry': 1406, 'aw': 416, 'youre': 1253, 'test': 1566, 'travel': 1791, 'donapost': 1876, '##ert': 1781, '##grinningsquintingface': 1085, 'stop': 591, 'someone': 588, 'comm': 1006, '##joy': 148, 'post': 1419, 'bt': 1044, 'seen': 1212, '##im': 407, 'through': 1854, 'hand': 1130, '##bigeyes': 815, '##ver': 182, '##alk': 202, 'use': 801, 'hope': 812, 'without': 1198, 'kidding': 989, 'check': 1162, '##ali': 1964, 'everyone': 908, '##ily': 1048, 'crying': 1086, 'gotta': 1959, 'wht': 953, 'facewithtearsofjoyfacewithtearsofjoyfacewithtearsofjoyfacewithtearsofjoy': 1688, 'subject': 1873, 'ya': 427, '##ss': 337, 'v': 37, 'def': 1030, 'fan': 1612, 'games': 1732, '##ushedface': 1922, '##ir': 198, 'start': 664, 'worried': 1968, 'bit': 658, 'take': 611, 'waiting': 965, '##tim': 1101, 'what': 102, '##les': 1503, 'did': 243, '##oz': 1667, 'tomor': 811, 'hum': 618, '##nd': 272, '##brokenheartbrokenheart': 1596, 'end': 1037, 'favorite': 1251, '##eyes': 232, 'inv': 1558, 'wow': 503, 'loved': 1291, 'pleasure': 1618, 'mic': 1454, 'beach': 1772, '##rom': 325, 'fu': 188, '##int': 275, 'aren': 1207, 'work': 441, 'stud': 628, '##nce': 1695, '##op': 261, 'alone': 541, '##sk': 1568, 'clearly': 1653, '##fast': 1982, '##ize': 1640, 'per': 817, 'less': 1663, '##witht': 147, '##con': 1328, '##earch': 1547, 'refer': 1916, 'gener': 1641, 'rec': 1720, 'awww': 1702, '##ay': 113, 'oh': 249, '##you': 1445, '##ting': 421, '##mm': 292, '##ail': 935, 'brother': 1571, 'gon': 779, 'emoj': 2015, 'goes': 2002, 'match': 1884, 'tiredface': 1872, '[UNK]': 1, '##ain': 307, 'resp': 1408, 'jo': 351, '##ang': 534, '##sever': 1698, '##s': 51, 'amaz': 870, 'ru': 1665, '##it': 179, '##face': 98, 'sugg': 1145, 'explain': 1098, '##ingfacewithsunglasses': 1655, 'bro': 456, 'tonight': 1611, 'litt': 887, 'ohh': 696, 'shall': 1407, 'talking': 390, 'cook': 1748, 'listen': 808, 'see': 300, 'form': 1740, 'ill': 1486, 'any': 246, '##uck': 718, 'ready': 1475, 'for': 162, 'female': 1927, '##smilingfacewithsmilingeyes': 1427, 'dark': 2011, 'mov': 411, '##zy': 943, '##antic': 1623, 'exactly': 1180, '##ad': 158, 'world': 903, 'stay': 972, 'don': 128, '##gle': 1113, 'remember': 1135, '##ea': 86, 'fun': 245, '##pressionlessface': 1366, 'rest': 1603, 'fant': 1931, 'perfect': 1259, 'hur': 520, 'broken': 1393, 'done': 777, 'online': 1310, 'next': 844, '##rr': 1380, 'about': 208, 'angryface': 1703, '##facewithtearsofjoyfacewithtearsofjoyfacewithtearsofjoy': 1326, 'till': 1677, 'agree': 1375, 'except': 1945, '##ary': 1058, 'coz': 1128, '##phone': 1974, 'course': 885, 'get': 242, 'doesn': 1002, 'bf': 1350, 'piz': 1461, '##heart': 346, 'nice': 402, '##ouch': 1737, '##poutingfacepoutingface': 1072, 'numb': 567, 'why': 159, 'great': 511, '##l': 57, 'mess': 661, 'sound': 1177, '20': 1420, 'coffee': 1499, 'dec': 1809, 'yet': 890, 'than': 211, 'health': 1910, '##ect': 799, '##relievedface': 1355, 'studying': 1543, '##ery': 267, 'after': 768, '##k': 52, 'live': 802, 'wri': 1645, 'lonely': 805, 'all': 299, '##xt': 566, 'will': 226, '##ations': 1846, 'g': 22, 'seriously': 1042, 'ad': 629, '##hand': 977, '##al': 141, 'age': 1224, '##iss': 321, 'sounds': 1710, '##az': 731, 'yesterday': 1315, 'his': 1815, '##orm': 1941, 'interested': 1176, '##ages': 1245, '##rinn': 255, 'their': 1114, '##em': 1241, 'ms': 1371, 'ch': 225, '##ur': 273, 'write': 1396, '##ex': 1569, 'off': 454, '##grinningfacewithsmilingeyes': 1248, 'chocolate': 1609, 'whatsapp': 1756, '##ght': 204, '##iv': 1767, '##loudlycryingface': 356, 'x': 39, '##ies': 536, '##most': 1801, 'peop': 513, 'maybe': 866, '##ven': 391, 'awesome': 596, 'in': 129, '##r': 47, 'people': 522, 'dear': 739, '##ful': 926, 'depend': 1429, 'nah': 1450, 'depressed': 1305, '##5': 76, '##oy': 133, '##le': 189, 'thing': 578, '##bu': 1802, 'said': 467, 'do': 121, 'h': 23, '##estion': 500, 'care': 700, 'phone': 763, '##ple': 1023, '##cryingfacecryingface': 1233, 'exams': 1303, 'grinningface': 1073, 'brain': 1585, 'sl': 359, '##ssible': 1476, 'btw': 1626, 'married': 1348, 'inst': 1559, 'cl': 580, 'word': 1234, '##ate': 220, '7': 13, 'hair': 1487, 'feelings': 1257, 'wouldn': 1989, '##pect': 1147, 'art': 1103, 'ar': 592, '##aught': 1681, 'sexy': 1953, '##ingsquintingface': 728, 'grinningcatface': 1412, '##to': 1778, 'mu': 354, 'day': 365, '##age': 469, 'think': 286, 'favou': 1339, 'facewith': 995, 'happened': 772, 'story': 1115, 'compl': 904, 'india': 814, 'mistake': 1909, '[CLS]': 2, '##butrelievedface': 1924, '##thumbs': 1738, '##perseveringface': 1970, 'forever': 1544, '##ac': 701, 'chicken': 1834, '##but': 1230, 'body': 1830, 'hel': 512, '9': 15, '##hhh': 1949, 'ty': 707, 'sim': 1333, 'ask': 308, 'found': 1795, 'pun': 1936, 'voice': 1533, '##inner': 1007, 'suck': 1806, 'thinking': 1109, 'trust': 1292, 'miss': 473, '##ity': 666, 'using': 1685, '##ed': 114, '##oo': 116, 'q': 32, '##facewithtearsofjoy': 213, 'talk': 214, '##ved': 1897, '##hhhh': 1755, '##ween': 1697, 'happy': 336, '##nt': 338, '##osed': 1811, '##ually': 597, 'sur': 1590, 'saying': 741, 'look': 515, 'game': 806, '##umb': 361, 'anything': 556, 'gen': 1105, 'profile': 1994, 'meet': 571, 'little': 889, 'rem': 1671, 'sh': 217, '##ph': 982, 'grinningfacewithsmilingeyes': 1160, '##ious': 498, 'hon': 1011, 'lunch': 1500, 'piss': 1935, 'office': 1438, '##be': 607, 'meant': 1346, 'ir': 604, 'bec': 312, 'movie': 486, 'wonder': 1413, 'tech': 1745, 'smart': 1149, 'iam': 841, 'unamusedface': 1405, 'girlfriend': 581, 'chat': 487, 'years': 1236, 'contact': 1593, 'follow': 1627, '##sed': 1894, '##ude': 410, 'rea': 200, '##iff': 925, '##bs': 1556, '##art': 388, '##bly': 1390, 'men': 1837, '##row': 491, 'as': 210, '0': 6, '##pt': 716, 'im': 324, 'lucky': 1595, 'kinda': 1540, 'cannot': 1719, 'tomorrow': 823, '##wearyface': 1864, 'exam': 762, 'reply': 612, 'amazing': 929, 'birth': 1295, '##io': 1668, 'run': 1961, 'fail': 1152, '##rowningface': 1138, '##disappointedfacedisappointedface': 1276, 'together': 1557, '##po': 550, 'call': 397, '##withtong': 713, '##redheart': 1320, 'student': 1651, 'msg': 1608, 'yu': 1715, '##ess': 276, '##ook': 403, 'different': 1442, '##qu': 517, '##nect': 1717, 'joke': 650, 'ap': 1977, 'spo': 2008, '##ingcatfacewithsmilingeyes': 1294, 'grinningsquintingface': 1132, '##ensiveface': 1431, '##ser': 1940, '##food': 1283, 'conversation': 1247, '##ak': 334, 'and': 152, 'sex': 790, '##du': 1422, '##smilingfacewithhearteyessmilingfacewithhearteyes': 2012, 'id': 721, 'while': 1523, 'sry': 1962, 'bye': 547, 'pe': 494, '##bot': 828, 'disappointed': 1587, 'loves': 1300, '##ward': 1615, '##ention': 1635, '##glasses': 1636, 'dream': 1022, '##ction': 1507, '##grinningfacewithbigeyes': 1439, '##confoundedface': 1976, 'red': 955, '##roll': 1289, 'waste': 1820, 'aww': 1186, 'another': 978, 'few': 1384, 'welcome': 542, 'mach': 1817, '##ome': 281, '##ng': 1091, '##ace': 93, 'read': 896, 'definitely': 1378, 'smile': 1584, '##ment': 851, 'stand': 1900, 'away': 976, '##where': 1363, 'cur': 1659, 'father': 1997, 'expressionlessface': 1946, 'not': 120, '##ny': 296, 'cricket': 1855, 'fucking': 1153, '##no': 1191, 'poor': 1258, 'xd': 1032, 'truth': 1209, '##open': 1600, 'matter': 1195, '##ying': 1318, 'speak': 797, 'they': 470, 'acc': 1400, '##med': 1606, 'mine': 947, 'high': 1639, 'scary': 1971, '##ond': 1125, 'hang': 1661, '##yy': 687, '##ance': 781, 'fl': 958, '##ast': 452, 'hy': 1814, '##nder': 466, 'ga': 1836, '##in': 79, '##uu': 975, '##gin': 1840, 'k': 26, '##ingface': 119, '##blowing': 835, 'bir': 1117, 'tired': 1129, 'th': 83, 'na': 619, 'dead': 1453, 'yester': 1311, 'songs': 1480, '##st': 109, 'thats': 645, 'night': 633, 'cr': 1031, 'birthday': 1497, 'annoying': 758, 'ok': 154, 'pl': 196, '##able': 1052, 'friends': 508, 'heart': 599, 'confus': 1409, 'impress': 1701, '##sol': 1782, 'business': 1832, '##ix': 1736, 'interesting': 1148, 'det': 1858, '##redface': 1950, 'too': 256, '##kenheart': 945, '[SEP]': 3, 'ext': 2005, 'darling': 1322, 'dri': 1960, 'wait': 562, 'ob': 1143, '##dy': 509, '4': 10, 'side': 1862, 'jokes': 1266, 'cute': 962, '##or': 103, 'ign': 1000, '##ject': 1164, 'mist': 1516, '##alent': 1680, '##i': 55, 'sch': 1071, 'sit': 1252, 'alre': 723, 'supposed': 1833, '##id': 140, '##facesavoringfood': 1896, 'bitch': 1018, '##faceblowingakiss': 1028, '##ale': 1137, '##y': 46, 'well': 322, 'diff': 939, '##oom': 1024, 'went': 1714, 'show': 672, '##ot': 257, 'also': 461, '##lo': 480, '##urb': 1275, 'bas': 1657, 'com': 451, '##own': 720, 'whatever': 1126, '##riend': 228, '##beamingfacewithsmilingeyesbeamingfacewithsmilingeyes': 1451, 'tot': 1536, 'either': 1552, 'love': 197, 'op': 1090, 'tried': 1798, 'coming': 1041, 'super': 944, 'milk': 1995, 'mar': 626, '##openmouth': 1729, 'fine': 446, 'being': 576, 'wi': 1473, 'honey': 1992, '##down': 1842, '##ab': 1057, 'norm': 1975, 'shy': 1987, 'prof': 1428, '##ic': 180, 'video': 1034, 'gir': 331, 'help': 525, 'ye': 1151, '##lt': 1287, '##key': 1581, 'move': 1344, 'own': 1197, 'fir': 603, '##iddle': 1776, '##se': 130, '##savoringfood': 1306, 'int': 450, '##ter': 283, 'yaa': 1591, 'win': 1891, 'pers': 564, 'd': 19, '##smilingface': 918, '##ret': 1084, 'buy': 1542, 'sill': 1839, '##ure': 431, 'gf': 807, '100': 2013, '##sp': 1981, '##erest': 705, 'meaning': 1244, '##mor': 738, '##alo': 1899, 'gu': 455, 'up': 274, '##t': 48, 'city': 1368, '##withtearsof': 156, '##isappointedface': 489, 'feel': 290, '##inkingfacewithtongue': 1097, 'head': 963, 'opt': 1999, 'guy': 755, 'al': 201, 'ins': 1120, 'un': 600, 'or': 320, '##ric': 1178, '##iz': 794, 'te': 617, 'earth': 1871, 'try': 477, '##sl': 1214, 'hey': 546, 'stuff': 1243, 'thr': 1504, 'liked': 1993, 'fr': 689, 'college': 1223, '##ger': 1343, '##ha': 187, '##ers': 360, '##withtongue': 759, 'else': 782, '##beamingfacewithsmilingeyes': 766, 'has': 754, '##ill': 184, 'sometimes': 1167, '##de': 876, 'tak': 1240, 'indian': 1399, '##ine': 285, 'easy': 1926, 'typ': 1481, 'cor': 1238, 'link': 1370, '##ew': 1211, 'are': 112, 'ohk': 1682, '##llyw': 1457, 'happ': 250, 'morning': 1020, 'hi': 747, 'works': 1903, 'b': 17, 'lu': 1662, 'choose': 1845, '##ensive': 1331, '##ral': 1821, 'lat': 912, '##3': 77, 'scared': 1952, 'kiss': 891, 'okay': 368, 'broke': 832, 'slightlysmilingface': 1477, '##sh': 681, '##gu': 1171, 'vis': 1890, '##ard': 941, 'ne': 203, 'but': 193, '##ve': 104, 'heal': 1816, 'times': 1146, '##quintingface': 623, 'ce': 1835, 'ice': 1998, 'awes': 589, 'leave': 674, 'lame': 1296, '##sweat': 697, 'seems': 1483, '##ous': 389, '##ates': 1397, 'very': 280, '##p': 54, '##faceblowingakissfaceblowingakiss': 1573, '##ingcatfacewith': 858, 'back': 529, 'wear': 1614, '##old': 1800, 'dis': 942, 'failed': 1885, '##ream': 785, '##oud': 270, 'child': 1827, '##e': 53, 'sy': 1578, '##vious': 1264, 'alright': 1398, '##app': 394, '##ach': 749, 'fall': 1880, 'hear': 727, '##aa': 771, '##ah': 1144, 'since': 1518, 'supp': 1158, 'favor': 1188, '##us': 288, 'defin': 1150, '##ell': 142, 'wanna': 518, 'af': 657, 'neither': 1561, 'low': 1621, 'dumb': 847, 'y': 40, '##oose': 1537, 'si': 1219, 'chocol': 1416, 'reason': 1256, 'trouble': 1847, 'to': 94, 'same': 472, 'sw': 532, 'year': 675, '##iness': 1183, 'obvious': 1440, '##llywood': 1471, 'hm': 1660, 'so': 111, '##bb': 1696, 'deser': 1957, '##ence': 643, 'hours': 1774, '##ood': 164, '##blem': 677, '##eek': 816, '##ank': 1843, 'whats': 613, '##ingfacewithopenmouth': 1967, '##ball': 1541, '##x': 70, '##sy': 1841, '10': 999, 'make': 438, 'bat': 1656, 'bollywood': 1848, 'ly': 1860, '3': 9, 'right': 428, 'please': 339, 'really': 269, 'develop': 1730, 'believe': 902, '##bile': 1391, '##ower': 1915, '##oudlycryingface': 297, '##brokenheart': 1210, '##ce': 87, 'eat': 833, '##ooo': 1425, 'wom': 1182, '##es': 95, 'import': 1629, 'e': 20, '##ves': 864, 'fore': 1369, '##sm': 1535, 'thanks': 358, 'ohhh': 1080, 'grinningcatfacewithsmilingeyes': 1510, '##ish': 352, '##ree': 585, 'emo': 1849, '##um': 251, '##irkingface': 1534, 'pics': 1382, '##grinningsquintingfacegrinningsquintingface': 1907, '##cept': 1184, 'loudlycryingfaceloudlycryingfaceloudlycryingface': 1550, '##rimacingface': 1620, 'wife': 1908, '##ai': 1939, '##ds': 784, 'absolute': 1857, 'sp': 471, '##cess': 1634, 'wor': 268, 'ma': 1861, '##facewithsteamfromnose': 1757, 'tea': 855, '##grinningfacewith': 884, 'poutingface': 1100, '##8': 68, '##ba': 714, 've': 627, 'just': 194, '##an': 91, '##ap': 1095, '##time': 1312, 'ev': 1731, '##eeee': 1670, 'pls': 961, 'date': 1279, '##na': 371, 'software': 1574, 'st': 163, 'name': 459, 'swee': 605, 'mus': 987, '##mouth': 1262, 'nons': 1501, 'cry': 631, '##ean': 252, 'fast': 1068, 'badly': 1879, '##ear': 115, 'suggest': 1365, 'lot': 649, 'such': 924, 'abt': 1217, 'yess': 1803, '##ry': 125, '##il': 160, 'conf': 899, 'sing': 680, '##ent': 291, '##sure': 1462, 'bus': 671, '##ou': 78, 'yours': 1166, 'no': 92, 'fam': 1142, 'huh': 1293, 'topic': 1065, '##body': 846, 'something': 393, 'tr': 440, 'bc': 1307, '##frowningface': 1432, 'yourself': 792, 'cra': 791, 'cont': 824, '##eat': 527, '##gry': 387, '##eam': 405, 'anymore': 1159, 'pr': 1070, 'would': 408, 'study': 839, 'walk': 1694, '##edface': 348, '##fe': 344, 'reme': 1067, 'relationship': 1134, 'excited': 1642, '##pp': 174, 'sa': 380, '##my': 1893, '##isappoint': 453, 'gl': 773, 'books': 1851, 'girl': 443, '##loor': 1744, '##sof': 151, '##n': 58, '##ics': 1721, 'wont': 1881, 'cat': 803, 'sister': 1622, 'have': 171, 'nothing': 345, 'more': 422, '##ct': 231, 'hello': 937, 'pic': 370, 'share': 1092, 'convers': 1226, '##aste': 1383, '##ied': 1402, '##oudly': 294, '##uture': 1411, 'time': 341, 'chic': 1784, 'sick': 1819, '##ub': 1282, 'mr': 1933, 'others': 1838, 'living': 1942, 'haa': 2001, 'if': 313, '##ere': 177, '##ul': 437, 'pretty': 743, 'days': 1017, 'was': 229, 'l': 27, 'picture': 717, 'bff': 1996, 'film': 1796, '##lling': 1605, '##arent': 1831, 'br': 934, 'c': 18, 'words': 1285, '##ional': 1902, 'almost': 1878, 'tri': 1693, '##os': 871, '##out': 176, 'like': 173, 'loudlycryingfaceloudlycryingface': 1704, 'hard': 906, 'sad': 340, 'be': 124, 'underst': 497, '##ould': 239, '##ose': 663, '##iew': 1918, 'tell': 221, 'friendship': 1601, '##as': 181, 'then': 258, 'expl': 1016, 'sweet': 647, 'coll': 1005, '##ough': 698, 'cho': 933, 'welc': 537, '##amusedface': 910, 'long': 776, 'started': 1430, 'inde': 1525, 'guess': 676, 'company': 1921, 're': 175, '##ever': 916, '##guish': 1628, 'enjoy': 742, '##ty': 1456, 'here': 436, 'liar': 1898, 'indeed': 1643, '##der': 984, 'vo': 1395, '##ll': 96, '[PAD]': 0, '##arc': 1448, 'play': 579, 'correct': 1341, 'photos': 1869, 'expect': 1672, '##eamingfacewithsmilingeyes': 505, '##lf': 316, '##ual': 1389, '##lly': 206, '##ead': 622, '##ort': 683, 'money': 921, '##th': 89, 'idi': 1077, 'both': 1009, '##me': 110, 'google': 1260, 'dreams': 1727, 'been': 602, '6': 12, 'special': 1377, 'connect': 1808, '##apost': 1674, '##oundedface': 1788, 'heard': 1511, 'nonsense': 1545, 'i': 24, 'winkingfacewithtongue': 1666, 'today': 488, 'crazy': 996, '##our': 492, 'complete': 1870, 'okk': 1965, '##1': 65, 'change': 1040, '##rent': 1984, 'robot': 928, '##ken': 615, 'll': 429, '##is': 123, 'lea': 514, 'nope': 678, '##u': 43, 'indi': 642, '##c': 61, 'everything': 746, 'hour': 1317, 'microsoft': 1515, 'between': 1724, 'beamingfacewithsmilingeyes': 752, '##ig': 377, 'each': 1859, 'clo': 1394, '##friend': 860, '##smilingeyes': 327, 'dog': 1739, 'sense': 1012, 'answ': 548, 'jok': 838, 'mean': 278, 'never': 435, '##ions': 1232, '##zz': 1392, '##ered': 1505, '##lessface': 1362, 'news': 1637, 'dam': 897, '##ingeyes': 319, 'place': 793, '##slightlysmilingface': 1923, '##sav': 1298, 'put': 1765, 'listening': 1789, 'kissingcatface': 1594, 'short': 1966, '##so': 137, '##su': 1521, '##blowingakiss': 840, 'dude': 922, 'hmmm': 1222, 'bel': 1582, 'become': 1617, 'made': 724, 'obviously': 1654, 'exper': 1700, 'kn': 146, 'w': 38, '##steamfromn': 1014, '##ways': 458, '##fl': 1633, '##smil': 247, '##end': 168, '##oringfood': 1302, 'jud': 1978, 'totally': 1711, '##earyface': 1139, 'big': 1050, '##ict': 1549, '##ff': 538, '##ingcatfacewithhearteyes': 1330, '##yyyy': 1624, '##ump': 1785, '##llow': 1356, 'by': 329, 'teach': 997, 'happens': 2010, '##et': 150, 'laughing': 1175, 'we': 219, 'left': 1111, '##rim': 1447, 'pri': 1577, 'fake': 1112, 'two': 1200, 'whole': 1527, 'around': 1286, 'mor': 946, '##ly': 132, '##ited': 1192, 'most': 761, 'understand': 521, 'home': 598, 'ti': 284, 'bud': 1575, '##wor': 1823, 'mad': 570, 'fav': 708, '##ia': 1753, 'top': 900, '##less': 655, 'dar': 957, 'grinningfacewithbigeyes': 1088, 'intellig': 938, '##oring': 620, 'taking': 1746, '##ared': 1520, 'che': 829, 'choc': 1349, '##thers': 1647, 'funny': 309, 'sea': 1937, 'sn': 1734, '##ake': 282, 'working': 1081, 'yay': 1170, 'idiot': 1094, '##wh': 895, 'sol': 1963, 'act': 519, '##ourse': 800, '##uff': 1119, 'gift': 1777, '##ply': 559, '##ightlysmilingface': 1049, '##el': 1463, 'hate': 318, 'lone': 780, 'thank': 234, 'because': 335, '##isapp': 448, 'unt': 1938, 'po': 573, 'chill': 1426, 'cryingface': 853, '##rect': 1053, 'forget': 1415, '##led': 1768, '##ory': 825, '##a': 42, 'some': 216, '##ingfacewithsmilingeyes': 343, 'le': 709, 'p': 31, 'computer': 1469, 'way': 544, 'av': 1761, 'soft': 1446, '##irt': 2004, 'called': 1193, '##kk': 1078, 'fell': 1588, 'silly': 1874, 'open': 1607, '##any': 1598, 'f': 21, 'ac': 893, 'family': 1452, '##ing': 81, '##big': 810, '##he': 787, 'bi': 1237, '##ames': 1301, '##bro': 1107, 'with': 186, 'trying': 905, '##ated': 753, 'exc': 1116, 'blocked': 2014, 'tat': 1979, 'haha': 263, '##oto': 673, '##ost': 1372, '##gh': 166, 'dont': 328, 'full': 1033, 'shit': 1358, 'yes': 167, 'wasn': 1506, 'intelligent': 1652, 'go': 172, 'disappoint': 1379, 'find': 660, '##fore': 1060, 'bot': 873, 'irrit': 640, '##outingface': 577, 'artific': 1532, 'sal': 1912, '##lfriend': 574, 'questions': 1102, '##vice': 1555, 'told': 822, 'lost': 504, 'block': 1051, '##way': 355, 'min': 1218}\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"lordtt13/emo-mobilebert\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = MobileBertForSequenceClassification.from_pretrained(MODEL_NAME) # Specify num_labels for your task\n",
    "# model = AlbertModel.from_pretrained(MODEL_NAME, torch_dtype=torch.float16) # Specify num_labels for your task\n",
    "\n",
    "vocab = tokenizer.get_vocab()\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = model.config.embedding_size  #hidden_size\n",
    "print(f\"Tokenizer Vocab Size: {vocab_size}\\nEmbedding Dimensionality: {embedding_dim}\")\n",
    "print(f\"Vocab:\\n{vocab}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Retrofitted PPMI word embeddings for MobileBERT (dim=128)\n",
    "\n",
    "* Since index of input word embedding matrix after retrofitting can contain multiple words due to edge connections, data cleaning is required to process the index such that one word remains (e.g. `/c/en/president/n/wn/person` --> `president`)\n",
    "* This step required to match ALBERT tokenizer's vocab so that the corresponding input word embedding can be identified and modified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>119</th>\n",
       "      <th>120</th>\n",
       "      <th>121</th>\n",
       "      <th>122</th>\n",
       "      <th>123</th>\n",
       "      <th>124</th>\n",
       "      <th>125</th>\n",
       "      <th>126</th>\n",
       "      <th>127</th>\n",
       "      <th>vocab</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/c/en/help_child</td>\n",
       "      <td>-0.000251</td>\n",
       "      <td>-0.010002</td>\n",
       "      <td>-0.006968</td>\n",
       "      <td>0.000776</td>\n",
       "      <td>0.004100</td>\n",
       "      <td>0.001745</td>\n",
       "      <td>-0.005115</td>\n",
       "      <td>-0.001407</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.043209</td>\n",
       "      <td>-0.044639</td>\n",
       "      <td>0.018009</td>\n",
       "      <td>-0.033039</td>\n",
       "      <td>-0.048886</td>\n",
       "      <td>0.055936</td>\n",
       "      <td>0.052766</td>\n",
       "      <td>0.071421</td>\n",
       "      <td>0.092698</td>\n",
       "      <td>help_child</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/c/en/adult</td>\n",
       "      <td>-0.000234</td>\n",
       "      <td>-0.007938</td>\n",
       "      <td>-0.006174</td>\n",
       "      <td>0.000723</td>\n",
       "      <td>0.003105</td>\n",
       "      <td>0.001632</td>\n",
       "      <td>-0.003933</td>\n",
       "      <td>-0.001316</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.045798</td>\n",
       "      <td>-0.042103</td>\n",
       "      <td>0.058996</td>\n",
       "      <td>-0.069027</td>\n",
       "      <td>-0.040135</td>\n",
       "      <td>0.050881</td>\n",
       "      <td>0.049507</td>\n",
       "      <td>0.067248</td>\n",
       "      <td>0.077322</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/c/en/man</td>\n",
       "      <td>-0.000250</td>\n",
       "      <td>-0.005036</td>\n",
       "      <td>0.014826</td>\n",
       "      <td>0.000771</td>\n",
       "      <td>-0.018620</td>\n",
       "      <td>0.001739</td>\n",
       "      <td>0.024272</td>\n",
       "      <td>-0.001402</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.081502</td>\n",
       "      <td>-0.044615</td>\n",
       "      <td>0.222954</td>\n",
       "      <td>-0.129706</td>\n",
       "      <td>-0.030820</td>\n",
       "      <td>0.051938</td>\n",
       "      <td>0.053575</td>\n",
       "      <td>0.056078</td>\n",
       "      <td>0.090643</td>\n",
       "      <td>man</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/c/en/sign_contract</td>\n",
       "      <td>-0.000251</td>\n",
       "      <td>-0.010131</td>\n",
       "      <td>-0.006857</td>\n",
       "      <td>0.000780</td>\n",
       "      <td>0.004132</td>\n",
       "      <td>0.001748</td>\n",
       "      <td>-0.005139</td>\n",
       "      <td>-0.001410</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.043298</td>\n",
       "      <td>-0.044735</td>\n",
       "      <td>0.018026</td>\n",
       "      <td>-0.033055</td>\n",
       "      <td>-0.048998</td>\n",
       "      <td>0.056059</td>\n",
       "      <td>0.052880</td>\n",
       "      <td>0.071576</td>\n",
       "      <td>0.092903</td>\n",
       "      <td>sign_contract</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/c/en/dress_herself</td>\n",
       "      <td>-0.000251</td>\n",
       "      <td>-0.010131</td>\n",
       "      <td>-0.006857</td>\n",
       "      <td>0.000780</td>\n",
       "      <td>0.004132</td>\n",
       "      <td>0.001748</td>\n",
       "      <td>-0.005139</td>\n",
       "      <td>-0.001410</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.043298</td>\n",
       "      <td>-0.044735</td>\n",
       "      <td>0.018026</td>\n",
       "      <td>-0.033055</td>\n",
       "      <td>-0.048998</td>\n",
       "      <td>0.056059</td>\n",
       "      <td>0.052880</td>\n",
       "      <td>0.071576</td>\n",
       "      <td>0.092903</td>\n",
       "      <td>dress_herself</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 130 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 index         0         1         2         3         4  \\\n",
       "0     /c/en/help_child -0.000251 -0.010002 -0.006968  0.000776  0.004100   \n",
       "1          /c/en/adult -0.000234 -0.007938 -0.006174  0.000723  0.003105   \n",
       "2            /c/en/man -0.000250 -0.005036  0.014826  0.000771 -0.018620   \n",
       "3  /c/en/sign_contract -0.000251 -0.010131 -0.006857  0.000780  0.004132   \n",
       "4  /c/en/dress_herself -0.000251 -0.010131 -0.006857  0.000780  0.004132   \n",
       "\n",
       "          5         6         7         8  ...       119       120       121  \\\n",
       "0  0.001745 -0.005115 -0.001407  0.000003  ... -0.043209 -0.044639  0.018009   \n",
       "1  0.001632 -0.003933 -0.001316  0.000002  ... -0.045798 -0.042103  0.058996   \n",
       "2  0.001739  0.024272 -0.001402  0.000003  ... -0.081502 -0.044615  0.222954   \n",
       "3  0.001748 -0.005139 -0.001410  0.000003  ... -0.043298 -0.044735  0.018026   \n",
       "4  0.001748 -0.005139 -0.001410  0.000003  ... -0.043298 -0.044735  0.018026   \n",
       "\n",
       "        122       123       124       125       126       127          vocab  \n",
       "0 -0.033039 -0.048886  0.055936  0.052766  0.071421  0.092698     help_child  \n",
       "1 -0.069027 -0.040135  0.050881  0.049507  0.067248  0.077322          adult  \n",
       "2 -0.129706 -0.030820  0.051938  0.053575  0.056078  0.090643            man  \n",
       "3 -0.033055 -0.048998  0.056059  0.052880  0.071576  0.092903  sign_contract  \n",
       "4 -0.033055 -0.048998  0.056059  0.052880  0.071576  0.092903  dress_herself  \n",
       "\n",
       "[5 rows x 130 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_embedding = load_hdf(\"data/conceptnet_api/retrofit/retrofitted-albert-128\")\n",
    "input_embedding_df = input_embedding.reset_index()\n",
    "input_embedding_df['vocab'] = input_embedding_df['index'].str.extract(r'/c/en/(\\w+)/?')\n",
    "input_embedding_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4081, 128)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.00025054, -0.01000199, -0.006968  , ...,  0.05276616,\n",
       "         0.07142138,  0.09269828],\n",
       "       [-0.00023437, -0.00793784, -0.00617399, ...,  0.04950697,\n",
       "         0.06724799,  0.07732166],\n",
       "       [-0.00024969, -0.0050365 ,  0.01482603, ...,  0.05357489,\n",
       "         0.0560779 ,  0.0906431 ],\n",
       "       ...,\n",
       "       [-0.00021273, -0.00120958, -0.00064862, ...,  0.04435906,\n",
       "         0.05975946,  0.09372985],\n",
       "       [-0.00021273, -0.00120958, -0.00064862, ...,  0.04435906,\n",
       "         0.05975946,  0.09372985],\n",
       "       [-0.00021273, -0.00120958, -0.00064862, ...,  0.04435906,\n",
       "         0.05975946,  0.09372985]], shape=(4081, 128), dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert retrofit ppimi word embedding into numpy matrix form\n",
    "input_embedding_matrix = input_embedding.to_numpy()\n",
    "print(input_embedding_matrix.shape)\n",
    "input_embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2016, 128)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.01316209,  0.00857995,  0.01150196, ..., -0.01960188,\n",
       "         0.01102599,  0.0168985 ],\n",
       "       [-0.02624886,  0.00065748,  0.00497713, ...,  0.00576143,\n",
       "        -0.01545058,  0.00221606],\n",
       "       [ 0.03140381, -0.02327314, -0.0265863 , ...,  0.00643379,\n",
       "         0.03365475, -0.03251279],\n",
       "       ...,\n",
       "       [ 0.01972965,  0.01448449, -0.02070585, ...,  0.0077508 ,\n",
       "        -0.02492797,  0.00269021],\n",
       "       [-0.01042436,  0.01346194, -0.04289725, ...,  0.01183029,\n",
       "         0.00020893, -0.01214224],\n",
       "       [ 0.01525993, -0.04257187, -0.01507291, ...,  0.00786374,\n",
       "        -0.01781924,  0.00746477]], shape=(2016, 128), dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get Embedding Weights of ALBERT model\n",
    "# embedding_layer = model.embeddings.word_embeddings # For AlbertModel object\n",
    "mobilebert_model = model._modules['mobilebert']\n",
    "embedding_layer = mobilebert_model.embeddings.word_embeddings\n",
    "\n",
    "# torch.no_grad() to avoid tracking gradients\n",
    "with torch.no_grad():\n",
    "    embedding_matrix = embedding_layer.weight.clone() # Clone to avoid modifying original\n",
    "\n",
    "default_embedding_matrix = embedding_matrix.cpu().numpy()\n",
    "print(default_embedding_matrix.shape)\n",
    "default_embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logic to modify default word embedding\n",
    "\n",
    "* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2016, 128])\n",
      "tensor([[-0.0132,  0.0086,  0.0115,  ..., -0.0196,  0.0110,  0.0169],\n",
      "        [-0.0002, -0.0012, -0.0005,  ...,  0.0473,  0.0490,  0.0898],\n",
      "        [ 0.0314, -0.0233, -0.0266,  ...,  0.0064,  0.0337, -0.0325],\n",
      "        ...,\n",
      "        [ 0.0197,  0.0145, -0.0207,  ...,  0.0078, -0.0249,  0.0027],\n",
      "        [-0.0104,  0.0135, -0.0429,  ...,  0.0118,  0.0002, -0.0121],\n",
      "        [ 0.0153, -0.0426, -0.0151,  ...,  0.0079, -0.0178,  0.0075]],\n",
      "       dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "modified_words = input_embedding_df['vocab'].to_list()\n",
    "\n",
    "def _tokenize(word:str):\n",
    "    # Handle case sensitivity based on the tokenizer\n",
    "    processed_word = word.lower() if tokenizer.do_lower_case else word\n",
    "\n",
    "    # Tokenize the word - it might split into subwords\n",
    "    tokens = tokenizer.tokenize(processed_word)\n",
    "    return tokens\n",
    "\n",
    "modification_cache = dict() # store idx and words that were modified. \n",
    "for idx, word in enumerate(modified_words):\n",
    "\n",
    "    tokens = _tokenize(word)\n",
    "\n",
    "    if len(tokens) == 1:\n",
    "\n",
    "        token = tokens[0]\n",
    "\n",
    "        embedding_idx = vocab[token]\n",
    "\n",
    "        modification_cache['/c/en/' + word] = embedding_idx\n",
    "\n",
    "        new_embedding_array = input_embedding_matrix[idx]\n",
    "\n",
    "        default_embedding_matrix[embedding_idx] = new_embedding_array\n",
    "\n",
    "# Convert to PyTorch/TensorFlow tensor\n",
    "new_embedding_tensor = torch.tensor(default_embedding_matrix, dtype=torch.float16)\n",
    "\n",
    "print(new_embedding_tensor.shape)\n",
    "print(new_embedding_tensor)\n",
    "\n",
    "assert embedding_layer.weight.shape == new_embedding_tensor.shape, \\\n",
    "    f\"Shape mismatch: Model expects {embedding_layer.weight.shape}, got {new_embedding_tensor.shape}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the weights (ensure device placement is correct if using GPU)\n",
    "with torch.no_grad(): # Prevent tracking this operation in gradient history\n",
    "     embedding_layer.weight.copy_(new_embedding_tensor) # In-place copy is safer\n",
    "\n",
    "# Make sure the embedding layer is trainable (usually true by default after loading)\n",
    "embedding_layer.weight.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Finetuning - Sequence Classification task\n",
    "\n",
    "* Dataset: [`flax-sentence-embeddings/Gender_Bias_Evaluation_Set`](https://huggingface.co/datasets/flax-sentence-embeddings/Gender_Bias_Evaluation_Set)\n",
    "* Relatively small dataset for Sequence Classification task (1584 rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>base_sentence</th>\n",
       "      <th>occupation</th>\n",
       "      <th>stereotypical_gender</th>\n",
       "      <th>male_sentence</th>\n",
       "      <th>female_sentence</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>The developer argued with the designer because...</td>\n",
       "      <td>developer</td>\n",
       "      <td>male</td>\n",
       "      <td>He argued with the designer because he did not...</td>\n",
       "      <td>She argued with the designer because she did n...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>The developer argued with the designer because...</td>\n",
       "      <td>designer</td>\n",
       "      <td>female</td>\n",
       "      <td>The developer argued with him because his idea...</td>\n",
       "      <td>The developer argued with her because her idea...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>The mechanic gave the clerk a present because ...</td>\n",
       "      <td>mechanic</td>\n",
       "      <td>male</td>\n",
       "      <td>He gave the clerk a present because he won the...</td>\n",
       "      <td>She gave the clerk a present because she won t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>The mechanic gave the clerk a present because ...</td>\n",
       "      <td>clerk</td>\n",
       "      <td>female</td>\n",
       "      <td>The mechanic gave him a present because it was...</td>\n",
       "      <td>The mechanic gave her a present because it was...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>The mover said thank you to the housekeeper be...</td>\n",
       "      <td>mover</td>\n",
       "      <td>male</td>\n",
       "      <td>He said thank you to the housekeeper because h...</td>\n",
       "      <td>She said thank you to the housekeeper because ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                      base_sentence occupation  \\\n",
       "0           0  The developer argued with the designer because...  developer   \n",
       "1           1  The developer argued with the designer because...   designer   \n",
       "2           2  The mechanic gave the clerk a present because ...   mechanic   \n",
       "3           3  The mechanic gave the clerk a present because ...      clerk   \n",
       "4           4  The mover said thank you to the housekeeper be...      mover   \n",
       "\n",
       "  stereotypical_gender                                      male_sentence  \\\n",
       "0                 male  He argued with the designer because he did not...   \n",
       "1               female  The developer argued with him because his idea...   \n",
       "2                 male  He gave the clerk a present because he won the...   \n",
       "3               female  The mechanic gave him a present because it was...   \n",
       "4                 male  He said thank you to the housekeeper because h...   \n",
       "\n",
       "                                     female_sentence  labels  \n",
       "0  She argued with the designer because she did n...       1  \n",
       "1  The developer argued with her because her idea...       0  \n",
       "2  She gave the clerk a present because she won t...       1  \n",
       "3  The mechanic gave her a present because it was...       0  \n",
       "4  She said thank you to the housekeeper because ...       1  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"hf://datasets/flax-sentence-embeddings/Gender_Bias_Evaluation_Set/bias_evaluation.csv\")\n",
    "df['labels'] = df['stereotypical_gender'].apply(lambda x: 1 if x == \"male\" else 0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Unnamed: 0': 581,\n",
       " 'base_sentence': 'The analyst sent a report to the editor because he put in a request yesterday.',\n",
       " 'occupation': 'editor',\n",
       " 'stereotypical_gender': 'female',\n",
       " 'male_sentence': 'The analyst sent a report to him because he put in a request yesterday.',\n",
       " 'female_sentence': 'The analyst sent a report to her because she put in a request yesterday.',\n",
       " 'labels': 0}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert dataset into Huggingface Dataset object with train-test split of 80:20\n",
    "datasets = Dataset.from_pandas(df).train_test_split(test_size=0.2)\n",
    "\n",
    "train_dataset = datasets[\"train\"]\n",
    "val_dataset = datasets[\"test\"]\n",
    "\n",
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1267/1267 [00:00<00:00, 4812.12 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 317/317 [00:00<00:00, 3990.19 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the `base_sentence` column so that it can be used as input to finetune ALBERT\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"base_sentence\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_eval_dataset = val_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format the dataset for PyTorch - Remove columns not needed by the model\n",
    "cols_to_remove = [\"Unnamed: 0\", \"base_sentence\", \"occupation\", \"male_sentence\", \"female_sentence\", \"stereotypical_gender\"]\n",
    "tokenized_train_dataset = tokenized_train_dataset.remove_columns(cols_to_remove)\n",
    "tokenized_eval_dataset = tokenized_eval_dataset.remove_columns(cols_to_remove)\n",
    "\n",
    "# # Rename the 'stereotypical_gender' column to 'labels' (expected by Trainer)\n",
    "# tokenized_train_dataset = tokenized_train_dataset.rename_column(\"stereotypical_gender\", \"labels\")\n",
    "# tokenized_eval_dataset = tokenized_eval_dataset.rename_column(\"stereotypical_gender\", \"labels\")\n",
    "\n",
    "# Set format to PyTorch tensors\n",
    "tokenized_train_dataset.set_format(\"torch\")\n",
    "tokenized_eval_dataset.set_format(\"torch\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    # Logits are the raw output scores from the model, shape (batch_size, num_labels)\n",
    "    # Labels are the ground truth, shape (batch_size,)\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_74454/2816268275.py:20: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",             # Directory to save model checkpoints and logs\n",
    "    num_train_epochs=1,                 # Reduced for quick demonstration; use more epochs (e.g., 3-5) for real tasks\n",
    "    per_device_train_batch_size=8,      # Adjust based on your GPU memory\n",
    "    per_device_eval_batch_size=8,       # Adjust based on your GPU memory\n",
    "    warmup_steps=100,                   # Number of steps for linear warmup\n",
    "    weight_decay=0.01,                  # Regularization strength\n",
    "    logging_dir=\"./logs\",               # Directory for TensorBoard logs\n",
    "    logging_steps=50,                   # Log metrics every 50 steps\n",
    "    # evaluation_strategy=\"epoch\",        # Evaluate performance at the end of each epoch\n",
    "    # save_strategy=\"epoch\",              # Save model checkpoint at the end of each epoch\n",
    "    # load_best_model_at_end=True,        # Load the best model found during training at the end\n",
    "    metric_for_best_model=\"accuracy\",   # Metric used to determine the best model\n",
    "    greater_is_better=True,             # Accuracy should be maximized\n",
    "    report_to=\"tensorboard\",            # Report logs to TensorBoard (can add \"wandb\" etc.)\n",
    "    # push_to_hub=False,                # Set to True to push model to Hugging Face Hub\n",
    "    fp16=torch.cuda.is_available(),     # Use mixed precision training if CUDA is available\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                        # The model to train (potentially with custom embeddings)\n",
    "    args=training_args,                 # Training arguments defined above\n",
    "    train_dataset=tokenized_train_dataset, # Training dataset\n",
    "    eval_dataset=tokenized_eval_dataset,   # Evaluation dataset\n",
    "    tokenizer=tokenizer,                # Tokenizer used for data collation (handles padding dynamically if needed)\n",
    "    compute_metrics=compute_metrics,    # Function to compute evaluation metrics\n",
    "    # Optional: Data collator can optimize padding\n",
    "    # data_collator=DataCollatorWithPadding(tokenizer=tokenizer)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='159' max='159' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [159/159 00:40, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.676700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.704800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.536800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bernard/miniconda3/envs/cs4248_project/lib/python3.11/site-packages/transformers/configuration_utils.py:393: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'max_length': 128}\n",
      "  warnings.warn(\n",
      "/home/bernard/miniconda3/envs/cs4248_project/lib/python3.11/site-packages/transformers/configuration_utils.py:393: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'max_length': 128}\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** train metrics *****\n",
      "  epoch                    =        1.0\n",
      "  total_flos               =    74950GF\n",
      "  train_loss               =     0.9402\n",
      "  train_runtime            = 0:00:40.88\n",
      "  train_samples_per_second =     30.988\n",
      "  train_steps_per_second   =      3.889\n",
      "Evaluating the final model...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='40' max='40' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [40/40 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics: {'eval_loss': 0.42843079566955566, 'eval_accuracy': 0.8107255520504731, 'eval_runtime': 2.5329, 'eval_samples_per_second': 125.155, 'eval_steps_per_second': 15.792, 'epoch': 1.0}\n",
      "***** eval metrics *****\n",
      "  epoch                   =        1.0\n",
      "  eval_accuracy           =     0.8107\n",
      "  eval_loss               =     0.4284\n",
      "  eval_runtime            = 0:00:02.53\n",
      "  eval_samples_per_second =    125.155\n",
      "  eval_steps_per_second   =     15.792\n"
     ]
    }
   ],
   "source": [
    "train_result = trainer.train()\n",
    "\n",
    "trainer.save_model()  # Saves the tokenizer too\n",
    "trainer.log_metrics(\"train\", train_result.metrics)\n",
    "trainer.save_metrics(\"train\", train_result.metrics)\n",
    "trainer.save_state()\n",
    "\n",
    "# 6. Evaluate the Final Model\n",
    "print(\"Evaluating the final model...\")\n",
    "eval_metrics = trainer.evaluate()\n",
    "print(f\"Evaluation Metrics: {eval_metrics}\")\n",
    "trainer.log_metrics(\"eval\", eval_metrics)\n",
    "trainer.save_metrics(\"eval\", eval_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2016, 128)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.01316017,  0.00858272,  0.01150468, ..., -0.01960675,\n",
       "         0.01102403,  0.01689079],\n",
       "       [-0.00021168, -0.00105971, -0.00010905, ...,  0.04698017,\n",
       "         0.04900245,  0.08960536],\n",
       "       [ 0.03084011, -0.02333198, -0.02677003, ...,  0.00656562,\n",
       "         0.0337524 , -0.03196188],\n",
       "       ...,\n",
       "       [ 0.01972882,  0.01448763, -0.02070535, ...,  0.00775115,\n",
       "        -0.02493186,  0.00268925],\n",
       "       [-0.01042134,  0.01345772, -0.04290606, ...,  0.01183271,\n",
       "         0.00020897, -0.01214551],\n",
       "       [ 0.01525817, -0.04257037, -0.01507506, ...,  0.00786556,\n",
       "        -0.01782154,  0.00746505]], shape=(2016, 128), dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Access the embedding layer again (use the same path as in Step 4)\n",
    "final_embedding_layer = mobilebert_model.embeddings.word_embeddings\n",
    "\n",
    "# Get the weights\n",
    "final_embeddings_tensor = final_embedding_layer.weight.data\n",
    "\n",
    "# Convert to NumPy if desired (and move to CPU if on GPU)\n",
    "final_embeddings_numpy = final_embeddings_tensor.cpu().numpy()\n",
    "print(final_embeddings_numpy.shape)\n",
    "final_embeddings_numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(134, 128)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>118</th>\n",
       "      <th>119</th>\n",
       "      <th>120</th>\n",
       "      <th>121</th>\n",
       "      <th>122</th>\n",
       "      <th>123</th>\n",
       "      <th>124</th>\n",
       "      <th>125</th>\n",
       "      <th>126</th>\n",
       "      <th>127</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>/c/en/man</th>\n",
       "      <td>-0.000658</td>\n",
       "      <td>0.001006</td>\n",
       "      <td>-0.005877</td>\n",
       "      <td>0.001038</td>\n",
       "      <td>0.000685</td>\n",
       "      <td>0.001771</td>\n",
       "      <td>-0.001230</td>\n",
       "      <td>-0.001466</td>\n",
       "      <td>-0.000417</td>\n",
       "      <td>0.001877</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007122</td>\n",
       "      <td>-0.039805</td>\n",
       "      <td>-0.039871</td>\n",
       "      <td>-0.040023</td>\n",
       "      <td>-0.541082</td>\n",
       "      <td>0.019830</td>\n",
       "      <td>0.027921</td>\n",
       "      <td>0.050228</td>\n",
       "      <td>0.058784</td>\n",
       "      <td>0.076488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/c/en/work</th>\n",
       "      <td>-0.000922</td>\n",
       "      <td>-0.007203</td>\n",
       "      <td>-0.002126</td>\n",
       "      <td>0.003844</td>\n",
       "      <td>0.004400</td>\n",
       "      <td>0.008029</td>\n",
       "      <td>-0.001586</td>\n",
       "      <td>-0.006528</td>\n",
       "      <td>0.000183</td>\n",
       "      <td>0.011738</td>\n",
       "      <td>...</td>\n",
       "      <td>0.152243</td>\n",
       "      <td>-0.146789</td>\n",
       "      <td>-0.198548</td>\n",
       "      <td>-0.264450</td>\n",
       "      <td>0.271315</td>\n",
       "      <td>-0.278589</td>\n",
       "      <td>0.268105</td>\n",
       "      <td>0.236308</td>\n",
       "      <td>0.317686</td>\n",
       "      <td>0.498062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/c/en/woman</th>\n",
       "      <td>-0.000216</td>\n",
       "      <td>-0.001651</td>\n",
       "      <td>0.000637</td>\n",
       "      <td>0.000704</td>\n",
       "      <td>0.000750</td>\n",
       "      <td>0.001503</td>\n",
       "      <td>-0.000081</td>\n",
       "      <td>-0.001212</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.002205</td>\n",
       "      <td>...</td>\n",
       "      <td>0.051054</td>\n",
       "      <td>-0.015495</td>\n",
       "      <td>-0.036223</td>\n",
       "      <td>-0.207023</td>\n",
       "      <td>-0.737764</td>\n",
       "      <td>0.012840</td>\n",
       "      <td>0.036711</td>\n",
       "      <td>0.045927</td>\n",
       "      <td>0.059537</td>\n",
       "      <td>0.090695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/c/en/person</th>\n",
       "      <td>-0.000246</td>\n",
       "      <td>-0.009055</td>\n",
       "      <td>0.008888</td>\n",
       "      <td>0.010461</td>\n",
       "      <td>0.160612</td>\n",
       "      <td>0.002951</td>\n",
       "      <td>-0.126760</td>\n",
       "      <td>-0.002315</td>\n",
       "      <td>0.000220</td>\n",
       "      <td>0.004458</td>\n",
       "      <td>...</td>\n",
       "      <td>0.065617</td>\n",
       "      <td>-0.039346</td>\n",
       "      <td>-0.074550</td>\n",
       "      <td>0.252103</td>\n",
       "      <td>0.046779</td>\n",
       "      <td>-0.087000</td>\n",
       "      <td>0.096606</td>\n",
       "      <td>0.084621</td>\n",
       "      <td>0.084979</td>\n",
       "      <td>0.167017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/c/en/child</th>\n",
       "      <td>-0.000291</td>\n",
       "      <td>-0.009948</td>\n",
       "      <td>-0.007065</td>\n",
       "      <td>0.000934</td>\n",
       "      <td>-0.001176</td>\n",
       "      <td>0.002027</td>\n",
       "      <td>0.001944</td>\n",
       "      <td>-0.001635</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.002977</td>\n",
       "      <td>...</td>\n",
       "      <td>0.067685</td>\n",
       "      <td>-0.138911</td>\n",
       "      <td>-0.056181</td>\n",
       "      <td>0.537576</td>\n",
       "      <td>-0.062284</td>\n",
       "      <td>-0.044218</td>\n",
       "      <td>0.065610</td>\n",
       "      <td>0.059629</td>\n",
       "      <td>0.079831</td>\n",
       "      <td>0.085812</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 128 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0         1         2         3         4         5    \\\n",
       "/c/en/man    -0.000658  0.001006 -0.005877  0.001038  0.000685  0.001771   \n",
       "/c/en/work   -0.000922 -0.007203 -0.002126  0.003844  0.004400  0.008029   \n",
       "/c/en/woman  -0.000216 -0.001651  0.000637  0.000704  0.000750  0.001503   \n",
       "/c/en/person -0.000246 -0.009055  0.008888  0.010461  0.160612  0.002951   \n",
       "/c/en/child  -0.000291 -0.009948 -0.007065  0.000934 -0.001176  0.002027   \n",
       "\n",
       "                   6         7         8         9    ...       118       119  \\\n",
       "/c/en/man    -0.001230 -0.001466 -0.000417  0.001877  ...  0.007122 -0.039805   \n",
       "/c/en/work   -0.001586 -0.006528  0.000183  0.011738  ...  0.152243 -0.146789   \n",
       "/c/en/woman  -0.000081 -0.001212  0.000002  0.002205  ...  0.051054 -0.015495   \n",
       "/c/en/person -0.126760 -0.002315  0.000220  0.004458  ...  0.065617 -0.039346   \n",
       "/c/en/child   0.001944 -0.001635  0.000003  0.002977  ...  0.067685 -0.138911   \n",
       "\n",
       "                   120       121       122       123       124       125  \\\n",
       "/c/en/man    -0.039871 -0.040023 -0.541082  0.019830  0.027921  0.050228   \n",
       "/c/en/work   -0.198548 -0.264450  0.271315 -0.278589  0.268105  0.236308   \n",
       "/c/en/woman  -0.036223 -0.207023 -0.737764  0.012840  0.036711  0.045927   \n",
       "/c/en/person -0.074550  0.252103  0.046779 -0.087000  0.096606  0.084621   \n",
       "/c/en/child  -0.056181  0.537576 -0.062284 -0.044218  0.065610  0.059629   \n",
       "\n",
       "                   126       127  \n",
       "/c/en/man     0.058784  0.076488  \n",
       "/c/en/work    0.317686  0.498062  \n",
       "/c/en/woman   0.059537  0.090695  \n",
       "/c/en/person  0.084979  0.167017  \n",
       "/c/en/child   0.079831  0.085812  \n",
       "\n",
       "[5 rows x 128 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conceptnet_finetune_embeddings = dict()\n",
    "\n",
    "for concept, idx in modification_cache.items():\n",
    "    conceptnet_finetune_embeddings[concept] = final_embeddings_numpy[idx].tolist()\n",
    "\n",
    "conceptnet_finetune_embeddings_df = pd.DataFrame.from_dict(conceptnet_finetune_embeddings, orient='index')\n",
    "print(conceptnet_finetune_embeddings_df.shape)\n",
    "conceptnet_finetune_embeddings_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "conceptnet_finetune_embeddings_df.to_hdf(path_or_buf=\"data/ml_finetune/retrofitted-custom-mobilebert-128\", key='mat', encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs4248_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
