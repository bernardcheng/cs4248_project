{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bernard/miniconda3/envs/cs4248_project/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from utils.formats import load_hdf\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from datasets import Dataset\n",
    "from transformers import PreTrainedModel, AutoModel, AutoTokenizer, MobileBertForSequenceClassification \n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers.configuration_utils import PretrainedConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Huggingface model - [MobileBERT](https://huggingface.co/docs/transformers/model_doc/mobilebert#transformers.MobileBertForSequenceClassification)\n",
    "\n",
    "* Input Embedding Dimensionality cannot be too big. \n",
    "* Standard Flavours of BERT-based transformer models have input dim of 768. PPMi + Retrofitting takes too long to produce input embedding vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer Vocab Size: 2016\n",
      "Embedding Dimensionality: 128\n",
      "Vocab:\n",
      "{'##grinningfacewith': 884, '##ind': 301, 'ba': 311, '##thing': 195, 'enjoying': 1249, '##ingcatfacewithsmilingeyes': 1294, '##apost': 1674, '##ere': 177, 'cold': 1562, 'become': 1617, 'indeed': 1643, 'human': 692, '##savoringfood': 1306, \"'\": 5, '##joy': 148, 'away': 976, 'can': 161, '##yy': 687, 'pass': 1517, '##az': 731, '##ver': 182, 'brother': 1571, 'women': 1958, 'rude': 523, '##ud': 264, 'made': 724, 'seriously': 1042, 'hi': 747, 'ready': 1475, '##fore': 1060, 'plan': 843, 'bed': 909, 'hel': 512, '##rite': 880, 'coll': 1005, 'class': 1361, '##owing': 767, '##isappointedface': 489, 'isn': 1265, '##ast': 452, '##press': 634, '##gether': 1508, 'blocked': 2014, 'gon': 779, 'ad': 629, 'mother': 1917, '4': 10, '##o': 49, 'poutingface': 1100, 'least': 1592, 'phone': 763, 'gonna': 795, 'wearyface': 1632, 'loud': 1553, 'next': 844, 'friend': 293, 'need': 385, '##ooo': 1425, '##ose': 663, 'take': 611, '##blem': 677, '##ine': 285, '##een': 1268, 'ab': 192, 'being': 576, 'us': 528, '##bb': 1696, 'math': 1554, 'expect': 1672, 'thinking': 1109, 'grinningface': 1073, '##ightly': 826, 'contact': 1593, 'mar': 626, 'k': 26, '##inner': 1007, 'wa': 507, '##ive': 317, '##times': 1155, 'hour': 1317, 'fl': 958, 'defin': 1150, '##ire': 1353, '##log': 1459, '##osed': 1811, 'easy': 1926, 'years': 1236, '##ven': 391, 'sl': 359, 'des': 1039, 'sexy': 1953, '##ro': 165, '##unamusedface': 1449, '##ort': 683, 'facewithtearsofjoyfacewithtearsofjoyfacewithtearsofjoy': 644, 'engl': 715, '##redface': 1950, 'bir': 1117, '##so': 137, 'woman': 1773, '##ation': 399, 'only': 415, '##oto': 673, '##erest': 705, 'clo': 1394, 'friendship': 1601, 'up': 274, 'dont': 328, 'off': 454, 'wht': 953, 'plea': 326, 'course': 885, '##ors': 1524, 'impress': 1701, 'experience': 1888, 'smilingfacewithhearteyes': 932, 'online': 1310, '##nect': 1717, 'absolutely': 1928, '##hands': 1625, 'count': 1154, 'sound': 1177, 'wrong': 583, '##oose': 1537, 'relation': 1064, 'also': 461, '##pped': 1742, '##or': 103, 'soft': 1446, '##relievedface': 1355, 'tired': 1129, 'headache': 1790, 'belong': 1887, 'laugh': 682, 'since': 1518, '##ightlyfrowningface': 1707, '##rent': 1984, 'joking': 1496, '##ay': 113, 'sad': 340, 'yess': 1803, 'bl': 726, '##6': 75, 'wear': 1614, 'shall': 1407, '##severingface': 1712, '##be': 607, 'tri': 1693, '##im': 407, '##ough': 698, 'prof': 1428, 'always': 474, 'try': 477, 'questions': 1102, 'walk': 1694, 'confused': 1886, 'birth': 1295, '##ingcatfacewith': 858, 'wat': 398, 'run': 1961, '##ve': 104, 'mu': 354, 'seem': 970, 'yaa': 1591, '##ical': 1173, '##ld': 190, '##hh': 468, 'see': 300, 'op': 1090, '##glasses': 1636, 'col': 1526, '##grinningfacewithbigeyes': 1439, 'sick': 1819, '##fac': 1229, '##outh': 979, 'city': 1368, 'great': 511, 'answer': 609, 'missing': 1360, '##one': 223, 'anymore': 1159, 'dream': 1022, 'coup': 1919, '##fast': 1982, 'because': 335, 'best': 479, 'complete': 1870, 'then': 258, 'yet': 890, '##lock': 964, 'means': 656, '##ber': 804, 'music': 1055, 'pick': 1493, '##aught': 1681, '##ize': 1640, 'forever': 1544, 'cric': 1749, '##eeee': 1670, 'fe': 735, 'aww': 1186, 'knew': 1319, 'girls': 1227, '##ouse': 1205, 'af': 657, 'en': 353, 'milk': 1995, '##idd': 1570, 'looking': 1063, 'che': 829, '##eal': 923, 'sill': 1839, '##om': 476, '##mes': 1059, '##sol': 1782, '##art': 388, 'wait': 562, 'fee': 289, 'ice': 1998, 'll': 429, '##unch': 1228, '##ot': 257, 'jo': 351, 'grinningcatfacewithsmilingeyes': 1510, '##ank': 1843, 'told': 822, 'pleasure': 1618, 'interest': 710, '##ft': 769, 'show': 672, '##ab': 1057, 'coff': 1492, 'enough': 919, '##c': 61, 'break': 778, 'correct': 1341, 'were': 601, '##quint': 621, '##ause': 305, '##fl': 1633, 'scary': 1971, '##line': 1163, 'fast': 1068, '##bo': 750, '##ual': 1389, '##oundedface': 1788, '##ustr': 1986, '##own': 720, 'cha': 1472, 'dear': 739, '##av': 968, 'didnt': 1359, 'nd': 1280, '##led': 1768, '##grinningfacewithsmilingeyes': 1248, 'mon': 632, '##ons': 861, 'world': 903, 'mistake': 1909, 'ask': 308, 'back': 529, 'say': 262, 'picture': 717, 'dec': 1809, 'nonsense': 1545, '##g': 44, 'company': 1921, 'ter': 1713, 'understand': 521, 'obvious': 1440, 'chocol': 1416, '##arent': 1831, 'honey': 1992, 'def': 1030, 'se': 373, 'work': 441, '##u': 43, '##kenheart': 945, 'whole': 1527, '##uture': 1411, '##oint': 383, 'kidding': 989, 'thought': 651, 'fact': 1786, 'dr': 1043, '##augh': 638, '3': 9, '##ice': 304, '9': 15, '##orm': 1941, 'loudlycryingface': 652, 'school': 1179, '##iz': 794, 'matter': 1195, '##pp': 174, 'sarc': 1519, 'buy': 1542, '##cept': 1184, '##00': 1646, 'till': 1677, '##with': 117, 'disappointed': 1587, 'computer': 1469, 'cur': 1659, '##rowningface': 1138, 'business': 1832, 'pic': 370, 'depress': 1054, 'appreci': 1929, 'micro': 1470, 'did': 243, 'or': 320, '##int': 275, 'bit': 658, 'hurts': 1235, '##vice': 1555, 'fu': 188, 'heal': 1816, 'n': 29, 'broke': 832, 'du': 1630, 'sec': 892, 'thumbsup': 1485, 'cute': 962, 'bec': 312, 'hell': 501, '##ingsquintingface': 728, '##to': 1778, '##aste': 1383, 'lovely': 1901, 'bus': 671, 'able': 1616, '##isappoint': 453, '##alk': 202, '##hhh': 1949, '##xt': 566, '##er': 90, 'first': 614, '##kk': 1078, 'for': 162, 'fore': 1369, '##bile': 1391, 'bff': 1996, '##am': 230, '##ingfacewithhearteyes': 637, 'dinner': 1036, 'ign': 1000, 'think': 286, '##tt': 442, 'baby': 595, 'get': 242, '##grinn': 447, '##ory': 825, 'another': 978, 'tel': 1990, 'from': 364, '##ti': 848, '##gram': 1140, '##facewithsteamfromnose': 1757, 'where': 302, 'asking': 686, 'stop': 591, 'cry': 631, 'guess': 676, 'an': 106, '##ag': 587, 'thats': 645, 'many': 695, 'typ': 1481, 'everyone': 908, 'thn': 1354, '##r': 47, 'de': 396, '##nt': 338, '##ition': 1824, 'doesn': 1002, '##ai': 1939, 'kn': 146, 'face': 224, 'lea': 514, '##ily': 1048, 'supposed': 1833, 'nobody': 1242, '[UNK]': 1, 'kid': 783, 'emot': 1689, 'gf': 807, 'check': 1162, 'pretty': 743, '##astic': 1437, '##ional': 1902, 'relievedface': 1335, 'knw': 1324, '##lo': 480, 'kind': 630, 'bi': 1237, 'fool': 1443, '##time': 1312, 'than': 211, 'facewithtearsofjoy': 265, 'yeah': 287, 'min': 1218, 'too': 256, 'cor': 1238, '##pressionlessface': 1366, 'possible': 1686, 'bro': 456, '##zz': 1392, '##rible': 1944, 'al': 201, '##redheart': 1320, '##ingfacewith': 222, 'fine': 446, 'telling': 1336, '##ali': 1964, '##butrelievedface': 1924, '##inn': 238, 'help': 525, 'saying': 741, 'pictures': 1638, 'serious': 665, '##any': 1598, 'buddy': 1792, 'why': 159, 'ahead': 1956, 'proba': 1436, 'hm': 1660, '##ilm': 1357, '##ich': 378, '##ow': 101, '##smilingfacewithhearteyessmilingfacewithhearteyes': 2012, 'but': 193, 'we': 219, 'with': 186, 'beautiful': 1019, 'move': 1344, '##au': 237, 'loudlycryingfaceloudlycryingfaceloudlycryingface': 1550, 'come': 419, 'girlfriend': 581, '##ret': 1084, '##oringfood': 1302, 'bas': 1657, 's': 34, '##oud': 270, 'mad': 570, '##ames': 1301, 'men': 1837, '##us': 288, '##poutingface': 736, '##rong': 551, 'missed': 1122, 'chill': 1426, '##antic': 1623, 'makes': 1025, '##ac': 701, 'aw': 416, 'photo': 679, '##rimac': 1610, 'voice': 1533, '##ourse': 800, 'your': 144, '##iff': 925, '##lieved': 863, 'low': 1621, '##reat': 413, '##ingeyes': 319, '##ited': 1192, '##ark': 1373, '##edface': 348, 'shut': 849, '##cent': 1464, '##asses': 1602, 'short': 1966, '##p': 54, '##set': 1108, 'coffee': 1499, '##he': 787, 'match': 1884, 'make': 438, 'please': 339, 'sw': 532, 'u': 36, 'star': 1769, 'language': 998, 'indi': 642, 'im': 324, '##self': 535, 'sister': 1622, '##hhhh': 1755, '##quintingface': 623, 'marry': 1406, 'tea': 855, '##du': 1422, 'real': 616, '##more': 1136, '##ch': 143, 'tot': 1536, 'day': 365, 'waste': 1820, 'already': 725, 'sometimes': 1167, '##ash': 1270, '##gle': 1113, 'cook': 1748, 'alright': 1398, 'un': 600, 'confus': 1409, '##earyface': 1139, 'happened': 772, 'hu': 786, '##ome': 281, 'drink': 1314, '1': 7, 'laughing': 1175, '##ard': 941, 're': 175, 'follow': 1627, '##id': 140, '##sof': 151, '##blowing': 835, 'out': 432, 'conversation': 1247, 'movie': 486, '##ond': 1125, '##brokenheart': 1210, '##m': 50, '##el': 1463, '##ics': 1721, 'child': 1827, '##arc': 1448, 'trying': 905, 'health': 1910, '##ain': 307, 'bad': 315, '##sever': 1698, 'won': 610, 'intell': 920, '##gl': 590, 'last': 867, 'shit': 1358, '##ess': 276, 'either': 1552, 'together': 1557, 'comput': 1376, 'ar': 592, 'use': 801, 'wanna': 518, 'angry': 445, '##ushedface': 1922, '##row': 491, 'open': 1607, 'nope': 678, '##ies': 536, '##wor': 1823, '##an': 91, 'reply': 612, '##ary': 1058, 'crush': 1401, 'microsof': 1498, '##iment': 1867, 'feelings': 1257, '##ually': 597, 'bc': 1307, '##b': 62, 'refer': 1916, 'tr': 440, '##rollingeyes': 1853, 'dist': 1104, 'cause': 950, 'food': 993, 'don': 128, 'their': 1114, 'sy': 1578, 'true': 881, 'whatsapp': 1756, 'tv': 1444, 'sounds': 1710, '##ning': 690, 'choice': 1619, '##ble': 404, 'soon': 949, '##su': 1521, '##grinningface': 1076, 'boys': 1904, 'more': 422, 'asked': 831, 'such': 924, '##nd': 272, 'play': 579, '##cl': 1983, '##able': 1052, '##man': 1297, '##ser': 1940, '##facesavoringfood': 1896, '##ould': 239, '##de': 876, '##ion': 212, '##der': 984, 'stay': 972, 'ill': 1486, 'propose': 1988, 'opin': 1852, '##smil': 247, 'mobile': 1434, '##ale': 1137, '##ral': 1821, 'id': 721, 'p': 31, 'time': 341, 'mess': 661, '##ty': 1456, 'birthday': 1497, '##ra': 376, '##is': 123, '##witht': 147, 'okay': 368, '##ture': 699, '##ise': 901, 'kill': 1692, 'chocolate': 1609, 'smilingface': 1948, 'through': 1854, 'vide': 951, '##app': 394, '##ouch': 1737, '##sy': 1841, '##ular': 1868, 'ye': 1151, 'god': 1216, '##ew': 1211, '##mm': 292, 'reading': 1754, 'bollywood': 1848, '##disappointedfacedisappointedface': 1276, 'as': 210, '##ift': 1271, '##hearteyes': 549, 'let': 420, 'very': 280, '##ead': 622, 'del': 1528, '##ass': 624, 'down': 894, '##ump': 1785, 'google': 1260, 'loved': 1291, 'tra': 1202, 'da': 1351, 'anyone': 1121, 'book': 1045, 'na': 619, '##loor': 1744, 'special': 1377, 'came': 1658, 'stupid': 375, '##ah': 1144, 'inde': 1525, '8': 14, 'char': 1435, 'sleeping': 1538, 'studying': 1543, 'act': 519, 'ag': 425, '##facewith': 980, '##gin': 1840, '##irt': 2004, '##umb': 361, '##ving': 531, 'married': 1348, '##ol': 878, '##ert': 1781, 'may': 554, 'anything': 556, 'two': 1200, 'few': 1384, 'right': 428, 'hope': 812, 'yo': 1567, 'tomor': 811, '##ute': 722, 'my': 126, 'am': 135, 'eating': 1889, 'supp': 1158, '##ric': 1178, '##food': 1283, '##po': 550, '##ens': 930, 'rem': 1671, 'hard': 906, 'intelligence': 1313, 'aren': 1207, 'cho': 933, '6': 12, 'sm': 737, '##fer': 1157, 'books': 1851, 'worried': 1968, 'do': 121, 'su': 233, '##ght': 204, 'bet': 481, '##ther': 384, 'something': 393, 'people': 522, 'fir': 603, 'smilingfacewithsmilingeyes': 1187, 'deep': 1930, 'gener': 1641, '##oo': 116, 'gl': 773, 'ce': 1835, 'goo': 1056, '##rom': 325, 'usually': 2009, 'said': 467, 'sea': 1937, '##j': 64, '##me': 110, '##ries': 1943, '##haha': 1290, '##smilingeyes': 327, '##ost': 1372, '##ore': 406, '##steamfromnose': 1035, 'cu': 1308, 'giving': 1708, '##7': 71, '[CLS]': 2, '##mouth': 1262, '##confoundedface': 1976, 'sugg': 1145, 'gone': 1676, 'luck': 913, 'youre': 1253, '##e': 53, 'weekend': 1482, 'emoj': 2015, 'win': 1891, 'conf': 899, 'at': 295, 'spec': 1083, 'film': 1796, 'suck': 1806, 'whom': 1381, 'team': 1863, '##ys': 1980, '##ically': 1805, 'att': 1199, 'wo': 1579, '##pend': 1323, 'whatever': 1126, 'trouble': 1847, '##sh': 681, 'thing': 578, '##eart': 271, 'bye': 547, '##9': 74, 'heart': 599, 'rain': 1911, 'gott': 1804, 'smart': 1149, '##uff': 1119, 'pls': 961, 'truth': 1209, 'stand': 1900, 'wanted': 1325, 'no': 92, 'date': 1279, 'haa': 2001, '##gg': 740, 'started': 1430, 'when': 367, '##ment': 851, 'lets': 1001, 'facewithtearsofjoyfacewithtearsofjoy': 654, 'like': 173, 'poor': 1258, 'pers': 564, 'moment': 1826, 'pr': 1070, 'inst': 1559, 'redheart': 1586, '##umbs': 879, '##v': 59, 'swee': 605, '##grinningsquintingface': 1085, 'excited': 1642, 'ref': 1985, 'q': 32, '##oring': 620, 'getting': 770, '##ie': 279, '##faceangry': 1728, '##facean': 1277, '##sav': 1298, 'pre': 495, 'fem': 1705, 'house': 1309, 'wri': 1645, 'program': 1168, 'person': 575, 'days': 1017, '##ction': 1507, 'ugly': 1367, '##pect': 1147, '##brokenheartbrokenheart': 1596, 'wonder': 1413, '##ting': 421, '##outingface': 577, 'romantic': 1691, 'put': 1765, 'lo': 138, 'mean': 278, 'find': 660, '##ou': 78, 'favou': 1339, 'msg': 1608, 'winkingface': 1799, 'yup': 809, 'sp': 471, '##bot': 828, 'friends': 508, '##x': 70, 'now': 218, 'chat': 487, '##nder': 466, '##ff': 538, 'while': 1523, '##eary': 917, '##oz': 1667, 'that': 131, 'one': 248, '##are': 639, 'sleepy': 1743, 'glad': 827, '##faceblowingakiss': 1028, 'liked': 1993, 'future': 1421, 'st': 163, 'funny': 309, '##di': 464, 'unamusedface': 1405, '##skint': 1775, 'ch': 225, 'ever': 332, 'send': 333, 'c': 18, 'lonely': 805, 'way': 544, 'sweet': 647, 'clearly': 1653, 'le': 709, 'alre': 723, 'dar': 957, '##ates': 1397, 'cool': 414, '##where': 1363, '##med': 1606, 'not': 120, '##earch': 1547, '##est': 240, '##hip': 1589, '##lc': 524, '##ke': 122, 'top': 900, 'mom': 1403, 'morning': 1020, '##ear': 115, '##io': 1668, 'sry': 1962, 'trust': 1292, '##ust': 178, '##sw': 392, 'you': 80, 'must': 1106, 'interesting': 1148, '##ound': 582, 'actually': 670, 'profile': 1994, 'fuck': 379, 'i': 24, 'went': 1714, 'saw': 1494, 'app': 760, 'fav': 708, '##inkingfacewithtongue': 1097, 'loves': 1300, '##reak': 719, 'name': 459, '##ance': 781, '##rinn': 255, 'video': 1034, 'e': 20, '##in': 79, 'leaving': 1829, 'coz': 1128, 'on': 139, 'smil': 462, 'v': 37, 'india': 814, 'join': 1509, '##ssible': 1476, 'mo': 207, '##2': 69, 'sure': 457, 'useless': 1906, '##ign': 1374, 'kinda': 1540, 'games': 1732, '##w': 63, 'office': 1438, 'other': 646, 'problem': 684, '##ar': 170, '##phone': 1974, 'hy': 1814, '##ention': 1635, 'want': 215, 'amaz': 870, '##ect': 799, 'brokenheart': 1947, '12': 1747, 'busy': 819, 'boring': 862, 'ear': 911, 'leave': 674, 'our': 883, '##z': 67, '##inkingface': 694, '##eyes': 232, 'faceblowingakiss': 1783, 'hand': 1130, '##ity': 666, 'ir': 604, 'opt': 1999, 'address': 1726, 'z': 41, 'guy': 755, 'tech': 1745, 'photos': 1869, 'compliment': 1955, 'big': 1050, 'kiss': 891, 'car': 1181, 'are': 112, '##ken': 615, 'yesterday': 1315, 'speak': 797, 'slightlysmilingface': 1477, 'oh': 249, 'thumbs': 1124, 'disappointedface': 788, 'party': 1385, 'bu': 837, '##poutingfacepoutingface': 1072, 'exact': 1110, 'mus': 987, 'how': 169, 'sh': 217, 'maybe': 866, 'fin': 1089, 'sn': 1734, 'nice': 402, 'have': 171, '##ding': 641, '##ming': 1716, '##rect': 1053, '##ub': 1282, 'ya': 427, '##heart': 346, '##ix': 1736, '##ingface': 119, '##uu': 975, '##re': 84, 'um': 1752, 'any': 246, '##at': 82, 'after': 768, '##ating': 584, 'year': 675, 'engine': 1512, 'forget': 1415, '##winkingfacewithtongue': 1718, '10': 999, 'yea': 253, '##ious': 498, '##ult': 1347, 'here': 436, 'hop': 765, 'bot': 873, 'smile': 1584, 'tak': 1240, 'call': 397, '##a': 42, 'bf': 1350, 'suggest': 1365, 'his': 1815, '##light': 2000, 'x': 39, '##ry': 125, 'exc': 1116, 'better': 553, 'under': 475, 'smilingcatfacewithhearteyes': 1684, 'thnx': 1925, 'got': 478, 'much': 362, 'irritating': 798, 'badly': 1879, '7': 13, '##frowningface': 1432, '##lling': 1605, 'thank': 234, '##old': 1800, 'ph': 434, 'favor': 1188, 'int': 450, '##pt': 716, 'differe': 1189, 'father': 1997, 'watching': 1161, 'chic': 1784, 'ignore': 1572, 'eyes': 1690, 'grinningfacewithsmilingeyes': 1160, 'tried': 1798, 'deser': 1957, 'si': 1219, '100': 2013, 'catfacewithtearsofjoy': 1495, 'lu': 1662, '##ket': 940, 'rom': 1410, 'and': 152, 'colle': 1196, '##yyyy': 1624, 'ext': 2005, 'hehe': 1061, 'reason': 1256, '##ream': 785, '##rim': 1447, 'upset': 1185, 'repl': 1560, 'suff': 1583, 'late': 1190, '##st': 109, 'sex': 790, '##ir': 198, 'him': 821, 'probably': 1513, 'depend': 1429, 't': 35, '##q': 66, 'the': 107, 'pri': 1577, '##k': 52, 'har': 775, 'every': 418, 'dat': 1762, 'sending': 1828, '##ng': 1091, '##outingcatface': 1844, '##iew': 1918, 'norm': 1975, 'give': 423, '##ady': 669, 'block': 1051, '##ific': 1074, 'cricket': 1855, 'mat': 856, 'spo': 2008, '##lie': 565, 'fan': 1612, 'piss': 1935, 'wasn': 1506, 'artificial': 1551, 'money': 921, 'wi': 1473, 'idi': 1077, 'those': 1206, '##pose': 1321, '##facewithtearsofjoyfacewithtearsofjoyfacewithtearsofjoy': 1326, 'grinningfacewithbigeyes': 1088, 'change': 1040, 'choc': 1349, '##thers': 1647, '##ally': 417, 'grinn': 363, 'almost': 1878, '##oudlycryingface': 297, 'sent': 875, 'travel': 1791, 'they': 470, 'comp': 852, 'g': 22, '##ages': 1245, 'choose': 1845, 'wan': 185, '##uck': 718, 'inter': 1172, 'enjoy': 742, 'own': 1197, 'yours': 1166, 'totally': 1711, '##act': 703, 'gotta': 1959, '##eam': 405, 'cl': 580, 'pain': 948, '##ple': 1023, 'interested': 1176, '##nce': 1695, '##ide': 545, 'ton': 1502, 'foot': 1763, '##eak': 744, 'subject': 1873, 'be': 124, 'point': 1169, 'wife': 1908, 'proper': 1969, 'brain': 1585, '##smilingface': 918, 'bea': 751, '##ery': 267, '##ats': 1418, 'ga': 1836, 'sha': 648, '##ool': 342, 'wt': 915, 'hello': 937, 'fa': 372, 'job': 757, 'sim': 1333, 'grinningcatface': 1412, 'know': 153, 'was': 229, '##oom': 1024, 'them': 625, 'ins': 1120, 'inf': 1488, 'r': 33, '##lievedface': 877, '##smilingfacewithsmilingeyes': 1427, 'share': 1092, 'quite': 1564, 'favorite': 1251, '##ean': 252, '##iful': 1008, 'ra': 1751, 'loudlycryingfaceloudlycryingface': 1704, 'absolute': 1857, '##reen': 1780, 'spe': 967, 'ho': 745, 'is': 118, '##alo': 1899, '##less': 655, 'mind': 685, 'ma': 1861, 'langu': 874, '##lly': 206, 'b': 17, '##ather': 1082, '##up': 266, 'piz': 1461, 'ev': 1731, 'bitch': 1018, 'dumb': 847, 'live': 802, 'intelligent': 1652, '##ace': 93, '##per': 561, '##ship': 1069, 'fr': 689, 'dri': 1960, '##ush': 865, 'secret': 1246, 'grinningsquintingface': 1132, '##no': 1191, 'even': 482, '##sweat': 697, '##wn': 956, 'whats': 613, 'awww': 1702, '##tiredface': 1954, 'form': 1740, 'fam': 1142, 'd': 19, 'uh': 1203, 'rec': 1720, '##ror': 1892, 'lot': 649, '##if': 430, 'ey': 1352, '##llyw': 1457, 'yr': 1766, '##n': 58, 'di': 1118, '[PAD]': 0, '##open': 1600, '##les': 1503, 'expl': 1016, 'worry': 1404, '##cing': 1822, '##l': 57, '##ying': 1318, '##ject': 1164, 'worst': 1649, 'prom': 1664, 'full': 1033, 'half': 1580, 'yu': 1715, '##steamfromn': 1014, 'el': 729, '##steam': 994, '##withtearsofjoy': 157, 'gud': 1576, 'unt': 1938, 'tell': 221, 'pa': 1764, '##ward': 1615, '5': 11, '##hand': 977, 'meet': 571, 'ass': 1284, '##ge': 277, 'explain': 1098, '##way': 355, 'ly': 1860, '##oudly': 294, 'later': 1038, '##se': 130, '##ital': 2003, '##other': 952, 'hmmm': 1222, 'forgot': 1531, 'ohhh': 1080, 'reme': 1067, 'frnd': 1466, 'high': 1639, '##bu': 1802, '##ever': 916, 'ap': 1977, 'head': 963, 'appre': 1905, '20': 1420, '##ri': 145, '##ible': 1263, '##day': 349, 'tht': 1669, '##face': 98, 'little': 889, '##dy': 509, '##smilingfacewithhearteyes': 988, 'pur': 1239, '##ild': 1490, '##faceblowingakissfaceblowingakiss': 1573, 'reg': 1877, 'hurt': 560, '##le': 189, 'learn': 1250, '##sl': 1214, 'thr': 1504, 'yourself': 792, 'litt': 887, 'dude': 922, 'catch': 1972, 'angryface': 1703, '##gh': 166, '##ply': 559, 'blue': 1883, 'test': 1566, 'ser': 555, 'would': 408, 'age': 1224, '##ensiveface': 1431, 'month': 1479, '##ved': 1897, '##ut': 134, 'hur': 520, '##slightlysmilingface': 1923, 'soo': 1225, 'kissingcatface': 1594, 'dark': 2011, 'umm': 1613, '##ryingface': 235, '##ight': 244, 'happ': 250, 'underst': 497, '##qu': 517, '##riend': 228, '##ww': 830, '##gr': 931, '##ed': 114, 'sol': 1963, 'eat': 833, '##ingcatface': 485, '##ds': 784, 'f': 21, 'numb': 567, 'things': 704, '##ering': 1254, '##ude': 410, 'nothing': 345, 'grinningfacewithsweat': 1010, '##ell': 142, 'facewithsteamfromnose': 1467, 'cr': 1031, 'if': 313, '##bly': 1390, 'turn': 1758, 'stuff': 1243, '##h': 45, 'cheat': 1015, 'cou': 818, 'tru': 1075, 'idea': 1123, 'problems': 1771, '##grinningfacewithsweat': 1304, 'teach': 997, 'tur': 1261, 'huh': 1293, 'sal': 1912, '##facewithtearsofjoyfacewithtearsofjoy': 357, '##ue': 412, '##ba': 714, 'fall': 1880, '##loudlycryingfaceloudlycryingfaceloudlycryingfaceloudlycryingface': 1221, '##vious': 1264, '##rowningfacewithopenmouth': 1973, 'living': 1942, 'y': 40, 'depends': 1565, 'bat': 1656, 'left': 1111, 'happy': 336, 'devel': 1722, 'heard': 1511, 'damn': 974, 'forg': 842, 'songs': 1480, 'boy': 572, '##ower': 1915, '##ish': 352, 'answ': 548, 'imag': 1807, 'watch': 516, 'broken': 1393, '##gn': 936, 'met': 1599, 'import': 1629, '##withtongue': 759, 'waiting': 965, 'inv': 1558, 'neither': 1561, '##ations': 1846, 'cuz': 1759, 'sun': 1866, 'fant': 1931, 'having': 1004, 'feel': 290, 'post': 1419, 'cryingface': 853, 'these': 1027, 'wont': 1881, 'om': 1934, 'ok': 154, '##ful': 926, 'sorry': 330, 'lear': 954, 'before': 1079, 'weird': 1458, '##bs': 1556, 'non': 1733, 'sense': 1012, 'study': 839, '##winkingface': 1678, 'ohk': 1682, '##iss': 321, 'he': 314, '##gry': 387, 'water': 1650, '##s': 51, '##eamingfacewithsmilingeyes': 505, 'still': 594, 'diff': 939, 'times': 1146, '##sun': 1597, 'failed': 1885, 'single': 1087, '##sk': 1568, '##li': 1269, 'about': 208, 'ann': 662, '##1': 65, 'comm': 1006, 'week': 850, 'pun': 1936, '##es': 95, '##as': 181, 'emo': 1849, '##ate': 220, 'mak': 688, 'th': 83, '##bl': 693, 'mood': 854, 'breakfast': 1991, '##ig': 377, 'hot': 1201, '##ny': 296, 'goes': 2002, 'others': 1838, '##na': 371, '##ial': 712, '0': 6, 'again': 510, 'pizza': 1514, 'wouldn': 1989, 'hon': 1011, 'ac': 893, 'else': 782, '##al': 141, '##sp': 1981, '##iness': 1183, 'over': 730, 'wow': 503, 'ob': 1143, 'myself': 969, '##ince': 1215, 'important': 1760, 'absol': 1825, '##urb': 1275, 'dam': 897, 'av': 1761, 'meant': 1346, 'keep': 813, 'making': 1133, 'what': 102, 'night': 633, 'should': 439, 'hor': 1387, 'obviously': 1654, 'story': 1115, '##sure': 1462, 'convers': 1226, '##ings': 401, 'develop': 1730, '##thumbs': 1738, 'ohh': 696, 'without': 1198, 'lame': 1296, 'watched': 1530, 'disturb': 1364, 'res': 869, 'prefer': 1787, '##ract': 1673, '##earsof': 155, '##the': 1895, 'sub': 1699, 'cant': 888, 'agree': 1375, '[SEP]': 3, '##end': 168, '##ya': 1913, '##roll': 1289, 'movies': 1062, 'yep': 1340, 'go': 172, 'sing': 680, 'h': 23, 'girl': 443, 'chatting': 991, 'lunch': 1500, 'disappoint': 1379, '##rimacingface': 1620, 'irrit': 640, '##all': 1281, 'plans': 1920, '##beamingfacewithsmilingeyes': 766, '##ingfacewithopenmouth': 1967, 'vis': 1890, '##fe': 344, '##vers': 983, 'grinningfacewith': 691, 'bo': 557, 'cannot': 1719, 'wor': 268, '##angu': 859, 'cat': 803, 'ty': 707, 'oo': 1455, 'talking': 390, '##beamingfacewithsmilingeyesbeamingfacewithsmilingeyes': 1451, '##ingfacewithsunglasses': 1655, 'all': 299, '##ers': 360, 'once': 1489, '##ex': 1569, 'number': 606, 'creat': 990, 'might': 1417, 'sk': 1273, 'place': 793, 'lik': 1231, 'long': 776, 'body': 1830, 'hair': 1487, '##gu': 1171, 'sur': 1590, '##op': 261, '##eep': 350, '##rown': 872, 'fucking': 1153, 'ro': 563, 'everything': 746, 'ha': 105, 'end': 1037, '##gs': 1334, 'eng': 1337, 'str': 1548, 'mine': 947, '##ong': 260, 'tomorrow': 823, '##sed': 1894, 'co': 209, 'guys': 1414, 'vo': 1395, 'remember': 1135, '##ill': 184, '##loudlycryingfaceloudlycryingface': 493, 'of': 149, 'so': 111, 'add': 992, '##ous': 389, '##withtong': 713, 'shy': 1987, 'today': 488, 'mic': 1454, 'seen': 1212, 'gu': 455, 'yaar': 1194, 'bt': 1044, 'silly': 1874, '##my': 1893, '##itely': 1338, 'idk': 1882, '##ware': 1433, 'hang': 1661, 'works': 1903, '##ile': 756, '##ared': 1520, 'each': 1859, 'facewith': 995, '##ha': 187, '##by': 496, '##usedface': 834, '##ct': 231, '##friend': 860, '##lf': 316, 'yester': 1311, 'love': 197, '##cryingface': 254, 'jok': 838, '##ried': 789, 'some': 216, '##ightlysmilingface': 1049, '##but': 1230, '##iddle': 1776, 'female': 1927, 'its': 369, 'pe': 494, 'com': 451, 'honest': 1484, 'm': 28, 'exam': 762, 'te': 617, 'write': 1396, 'happen': 533, 'bel': 1582, '##8': 68, '##on': 88, 'self': 1029, '##ly': 132, 'ru': 1665, 'okk': 1965, 'sat': 1818, 'done': 777, 'art': 1103, '##blowingakiss': 840, 'does': 552, 'trash': 2007, 'winkingfacewithtongue': 1666, 'had': 558, 'pu': 1750, 'btw': 1626, 'hw': 1932, 'look': 515, 'game': 806, 'imp': 1208, 'compl': 904, '##big': 810, '##ird': 1274, 'in': 129, 'sch': 1071, 'crying': 1086, '##za': 1423, 'will': 226, 'message': 971, '##ange': 898, 'per': 817, 'to': 94, 'her': 569, 'question': 530, 'cra': 791, '##irkingface': 1534, 'peop': 513, 'die': 1794, 'haven': 1255, 'second': 1850, '##ter': 283, 'dis': 942, '##ea': 86, 'home': 598, 'college': 1223, '##loudlycryingface': 356, 'super': 944, '##openmouth': 1729, 'dance': 1267, '##right': 1131, 'exams': 1303, 'less': 1663, 'artific': 1532, 'fail': 1152, 'just': 194, '##ite': 543, '##ick': 706, '##oy': 133, 'listening': 1789, '##isapp': 448, '##llow': 1356, '##age': 469, 'who': 259, 'alone': 541, 'looks': 1478, 'pl': 196, '##estion': 500, 'veg': 1951, '##wearyface': 1864, 'donapost': 1876, 'well': 322, 'iam': 841, 'read': 896, 'microsoft': 1515, 'hear': 727, '##ways': 458, '##ey': 191, 'meaning': 1244, 'hate': 318, 'pro': 347, 'beamingfacewithsmilingeyes': 752, 'bored': 1141, 'dp': 1278, 'stud': 628, 'song': 635, 'except': 1945, '##ad': 158, 'o': 30, '##ink': 227, '##most': 1801, 'part': 748, '##ress': 502, '##ete': 1288, 'lat': 912, '##mil': 205, '##llywood': 1471, 'thanks': 358, 'sa': 380, 'li': 127, '##body': 846, 'good': 183, 've': 627, 'anyway': 1345, '##ated': 753, 'feeling': 506, 'mov': 411, 'there': 381, 'cont': 824, 'j': 25, '##iv': 1767, '##f': 60, 'resp': 1408, 'l': 27, 'she': 540, 'cheated': 1468, 'sit': 1252, 'topic': 1065, '##3': 77, 'a': 16, 'hmm': 499, 'lie': 1299, 'ms': 1371, '##lieve': 586, '##ood': 164, '##ss': 337, 'side': 1862, 'crazy': 996, 'this': 236, '##ip': 608, 'gen': 1105, 'mist': 1516, '##ari': 1204, '##et': 150, '##con': 1328, 'acc': 1400, 'sleep': 463, 'hurting': 1810, '##pensiveface': 1914, 'someone': 588, '##ions': 1232, '##ach': 749, 'called': 1193, '##red': 539, 'robot': 928, '##down': 1842, 'det': 1858, 'idiot': 1094, 'happens': 2010, '##ist': 483, '##out': 176, 'list': 732, 'w': 38, '##ake': 282, 'earth': 1871, 'lone': 780, 'xd': 1032, '##th': 89, '##wh': 895, 'playing': 1687, 'personal': 1770, '##steamfrom': 1013, '##perseveringface': 1970, 'link': 1370, '##grinningsquintingfacegrinningsquintingface': 1907, '##our': 492, '##rit': 465, '##ling': 1026, '##cat': 433, '##use': 1735, 'working': 1081, '##ience': 1156, '##you': 1445, '##cess': 1634, 'eas': 1644, '##ang': 534, 'news': 1637, 'qu': 395, '##ache': 1706, 'welc': 537, 'old': 868, 'darling': 1322, 'definitely': 1378, 'country': 1441, 'wtf': 1812, 'coming': 1041, 'nat': 1631, 'words': 1285, '##ce': 87, 'rea': 200, '##amusedface': 910, 'using': 1685, 'joke': 650, '##mor': 738, 'annoy': 667, 'ex': 306, '##apo': 981, '[MASK]': 4, 'hav': 1865, '##em': 1241, 'family': 1452, '##ph': 982, 'favourite': 1386, 'exist': 2006, '##ied': 1402, '##ensive': 1331, 'search': 1604, 'sor': 323, 'which': 400, '##ves': 864, '##blowingak': 836, '##ister': 1539, 'annoying': 758, 'fake': 1112, '##oot': 1329, 'found': 1795, '##ure': 431, 'it': 108, '##ack': 1213, 'yay': 1170, '##irk': 1465, '##ount': 1679, '##lfriend': 574, '##quintingfacewithtongue': 1725, 'type': 1093, 'stup': 374, 'awes': 589, '##d': 56, 'start': 664, '##its': 1491, 'scared': 1952, '##ree': 585, '##5': 76, '##i': 55, 'awesome': 596, 'free': 959, '##pe': 366, 'life': 424, '##guish': 1628, '##catface': 460, 'exactly': 1180, 'between': 1724, '##0': 72, 'hum': 618, '##sc': 1779, 'dare': 1813, 'new': 593, 'dreams': 1727, 'intern': 1709, 'em': 820, 'me': 99, 'dnt': 1316, '##y': 46, '##alent': 1680, 'expressionlessface': 1946, '##oooo': 1127, 'indian': 1399, 'wh': 85, 'doing': 490, '##bigeyes': 815, 'tw': 907, '##ouble': 1546, 'ti': 284, '##ath': 636, 'calling': 1723, 'dog': 1739, 'text': 986, 'dead': 1453, '##ia': 1753, 'ah': 1021, '##ence': 643, '##eek': 816, 'bud': 1575, 'gir': 331, 'br': 934, 'wom': 1182, 'connect': 1808, 'mr': 1933, 'software': 1574, '##mber': 960, '##and': 303, 'taking': 1746, '##son': 1047, 'fo': 1099, 'room': 1388, 'mil': 1797, 'sc': 653, 'really': 269, 'jokes': 1266, 'miss': 473, '##lt': 1287, '##ee': 136, 'seems': 1483, 'haha': 263, '##zy': 943, 'around': 1286, '##ps': 1066, '##pl': 449, 'nt': 1332, 'nons': 1501, 'rest': 1603, 'facewithtearsofjoyfacewithtearsofjoyfacewithtearsofjoyfacewithtearsofjoy': 1688, 'student': 1651, '##from': 1003, 'yes': 167, 'gr': 966, '##ween': 1697, '##ook': 403, '##ither': 985, '##lessface': 1362, '##ame': 241, 'prett': 733, 'talk': 214, 'ur': 298, 'lost': 504, 'could': 668, 'by': 329, '2': 8, 'though': 845, '##ored': 973, 'been': 602, 'gi': 1046, '##vel': 927, '##skintone': 1856, 'nor': 1424, 'mor': 946, '##cryingfacecryingface': 1233, 'pics': 1382, 'both': 1009, 'liar': 1898, '##ll': 96, 'intellig': 938, 'plz': 857, '##fect': 1096, '##ingfacewithsmilingeyes': 343, '##tim': 1101, '##bro': 1107, '##orn': 1165, 'nah': 1450, '##t': 48, 'po': 573, 'clear': 1174, '##ict': 1549, 'same': 472, '##il': 160, '##ing': 81, 'boyfriend': 882, 'relationship': 1134, 'chicken': 1834, 'amazing': 929, 'exper': 1700, '##ese': 886, 'wish': 711, 'far': 1342, 'ne': 203, 'welcome': 542, '##rr': 1380, '##disappointedface': 702, '##4': 73, 'fun': 245, 'con': 386, '##ail': 935, 'lucky': 1595, 'mach': 1817, '##ak': 334, 'beaut': 914, 'rel': 796, 'ug': 1327, 'jud': 1978, '##not': 1522, 'abt': 1217, 'different': 1442, 'man': 426, '##ness': 1474, '##ic': 180, '##un': 444, 'believe': 902, 'red': 955, 'word': 1234, '##ith': 100, 'hey': 546, 'going': 382, '##ul': 437, 'thou': 484, 'care': 700, '##pression': 1272, 'english': 734, '##eat': 527, '##ent': 291, '##ense': 764, '##ap': 1095, 'depressed': 1305, 'most': 761, 'into': 1529, 'tonight': 1611, 'state': 1741, '##ci': 1220, 'hours': 1774, 'listen': 808, '##it': 179, 'fell': 1588, 'says': 1683, '##um': 251, 'gift': 1777, 'ai': 774, '##en': 97, 'has': 754, '##ck': 199, 'used': 1460, '##ger': 1343, 'perfect': 1259, '##ball': 1541, '##conf': 1675, '##aa': 771, 'lol': 409, '##withtearsof': 156, 'ca': 310, 'hahaha': 659, 'didn': 526, '##ered': 1505, 'war': 1875, '##os': 871, '##key': 1581, 'babe': 1563, '##ingfacewithsun': 1648, 'beach': 1772, 'tat': 1979, 'suffering': 1793, '##sm': 1535, '##ingcatfacewithhearteyes': 1330, '##ant': 568, 'never': 435, '##facewithtearsofjoy': 213, '##ur': 273, 'tiredface': 1872}\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"lordtt13/emo-mobilebert\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = MobileBertForSequenceClassification.from_pretrained(MODEL_NAME) # Specify num_labels for your task\n",
    "# model = AlbertModel.from_pretrained(MODEL_NAME, torch_dtype=torch.float16) # Specify num_labels for your task\n",
    "\n",
    "vocab = tokenizer.get_vocab()\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = model.config.embedding_size  #hidden_size\n",
    "print(f\"Tokenizer Vocab Size: {vocab_size}\\nEmbedding Dimensionality: {embedding_dim}\")\n",
    "print(f\"Vocab:\\n{vocab}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Retrofitted PPMI word embeddings for MobileBERT (dim=128)\n",
    "\n",
    "* Since index of input word embedding matrix after retrofitting can contain multiple words due to edge connections, data cleaning is required to process the index such that one word remains (e.g. `/c/en/president/n/wn/person` --> `president`)\n",
    "* This step required to match ALBERT tokenizer's vocab so that the corresponding input word embedding can be identified and modified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>119</th>\n",
       "      <th>120</th>\n",
       "      <th>121</th>\n",
       "      <th>122</th>\n",
       "      <th>123</th>\n",
       "      <th>124</th>\n",
       "      <th>125</th>\n",
       "      <th>126</th>\n",
       "      <th>127</th>\n",
       "      <th>vocab</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/c/en/chair_meeting</td>\n",
       "      <td>-0.001778</td>\n",
       "      <td>0.007031</td>\n",
       "      <td>0.002296</td>\n",
       "      <td>0.00364</td>\n",
       "      <td>0.004209</td>\n",
       "      <td>-0.007875</td>\n",
       "      <td>0.001394</td>\n",
       "      <td>0.006352</td>\n",
       "      <td>0.004029</td>\n",
       "      <td>...</td>\n",
       "      <td>0.146947</td>\n",
       "      <td>0.198373</td>\n",
       "      <td>-0.264603</td>\n",
       "      <td>0.27145</td>\n",
       "      <td>-0.278929</td>\n",
       "      <td>-0.268215</td>\n",
       "      <td>-0.235852</td>\n",
       "      <td>-0.317816</td>\n",
       "      <td>-0.498361</td>\n",
       "      <td>chair_meeting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/c/en/chairperson</td>\n",
       "      <td>-0.001778</td>\n",
       "      <td>0.007031</td>\n",
       "      <td>0.002296</td>\n",
       "      <td>0.00364</td>\n",
       "      <td>0.004209</td>\n",
       "      <td>-0.007875</td>\n",
       "      <td>0.001394</td>\n",
       "      <td>0.006352</td>\n",
       "      <td>0.004029</td>\n",
       "      <td>...</td>\n",
       "      <td>0.146947</td>\n",
       "      <td>0.198373</td>\n",
       "      <td>-0.264603</td>\n",
       "      <td>0.27145</td>\n",
       "      <td>-0.278929</td>\n",
       "      <td>-0.268215</td>\n",
       "      <td>-0.235852</td>\n",
       "      <td>-0.317816</td>\n",
       "      <td>-0.498361</td>\n",
       "      <td>chairperson</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/c/en/chair</td>\n",
       "      <td>-0.001778</td>\n",
       "      <td>0.007031</td>\n",
       "      <td>0.002296</td>\n",
       "      <td>0.00364</td>\n",
       "      <td>0.004209</td>\n",
       "      <td>-0.007875</td>\n",
       "      <td>0.001394</td>\n",
       "      <td>0.006352</td>\n",
       "      <td>0.004029</td>\n",
       "      <td>...</td>\n",
       "      <td>0.146947</td>\n",
       "      <td>0.198373</td>\n",
       "      <td>-0.264603</td>\n",
       "      <td>0.27145</td>\n",
       "      <td>-0.278929</td>\n",
       "      <td>-0.268215</td>\n",
       "      <td>-0.235852</td>\n",
       "      <td>-0.317816</td>\n",
       "      <td>-0.498361</td>\n",
       "      <td>chair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/c/en/chairperson/n</td>\n",
       "      <td>-0.001778</td>\n",
       "      <td>0.007031</td>\n",
       "      <td>0.002296</td>\n",
       "      <td>0.00364</td>\n",
       "      <td>0.004209</td>\n",
       "      <td>-0.007875</td>\n",
       "      <td>0.001394</td>\n",
       "      <td>0.006352</td>\n",
       "      <td>0.004029</td>\n",
       "      <td>...</td>\n",
       "      <td>0.146947</td>\n",
       "      <td>0.198373</td>\n",
       "      <td>-0.264603</td>\n",
       "      <td>0.27145</td>\n",
       "      <td>-0.278929</td>\n",
       "      <td>-0.268215</td>\n",
       "      <td>-0.235852</td>\n",
       "      <td>-0.317816</td>\n",
       "      <td>-0.498361</td>\n",
       "      <td>chairperson</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/c/en/president/n/wn/person</td>\n",
       "      <td>-0.001778</td>\n",
       "      <td>0.007031</td>\n",
       "      <td>0.002296</td>\n",
       "      <td>0.00364</td>\n",
       "      <td>0.004209</td>\n",
       "      <td>-0.007875</td>\n",
       "      <td>0.001394</td>\n",
       "      <td>0.006352</td>\n",
       "      <td>0.004029</td>\n",
       "      <td>...</td>\n",
       "      <td>0.146947</td>\n",
       "      <td>0.198373</td>\n",
       "      <td>-0.264603</td>\n",
       "      <td>0.27145</td>\n",
       "      <td>-0.278929</td>\n",
       "      <td>-0.268215</td>\n",
       "      <td>-0.235852</td>\n",
       "      <td>-0.317816</td>\n",
       "      <td>-0.498361</td>\n",
       "      <td>president</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 130 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         index         0         1         2        3  \\\n",
       "0          /c/en/chair_meeting -0.001778  0.007031  0.002296  0.00364   \n",
       "1            /c/en/chairperson -0.001778  0.007031  0.002296  0.00364   \n",
       "2                  /c/en/chair -0.001778  0.007031  0.002296  0.00364   \n",
       "3          /c/en/chairperson/n -0.001778  0.007031  0.002296  0.00364   \n",
       "4  /c/en/president/n/wn/person -0.001778  0.007031  0.002296  0.00364   \n",
       "\n",
       "          4         5         6         7         8  ...       119       120  \\\n",
       "0  0.004209 -0.007875  0.001394  0.006352  0.004029  ...  0.146947  0.198373   \n",
       "1  0.004209 -0.007875  0.001394  0.006352  0.004029  ...  0.146947  0.198373   \n",
       "2  0.004209 -0.007875  0.001394  0.006352  0.004029  ...  0.146947  0.198373   \n",
       "3  0.004209 -0.007875  0.001394  0.006352  0.004029  ...  0.146947  0.198373   \n",
       "4  0.004209 -0.007875  0.001394  0.006352  0.004029  ...  0.146947  0.198373   \n",
       "\n",
       "        121      122       123       124       125       126       127  \\\n",
       "0 -0.264603  0.27145 -0.278929 -0.268215 -0.235852 -0.317816 -0.498361   \n",
       "1 -0.264603  0.27145 -0.278929 -0.268215 -0.235852 -0.317816 -0.498361   \n",
       "2 -0.264603  0.27145 -0.278929 -0.268215 -0.235852 -0.317816 -0.498361   \n",
       "3 -0.264603  0.27145 -0.278929 -0.268215 -0.235852 -0.317816 -0.498361   \n",
       "4 -0.264603  0.27145 -0.278929 -0.268215 -0.235852 -0.317816 -0.498361   \n",
       "\n",
       "           vocab  \n",
       "0  chair_meeting  \n",
       "1    chairperson  \n",
       "2          chair  \n",
       "3    chairperson  \n",
       "4      president  \n",
       "\n",
       "[5 rows x 130 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_embedding = load_hdf(\"data/conceptnet_api/retrofit/retrofitted-albert-128\")\n",
    "input_embedding_df = input_embedding.reset_index()\n",
    "input_embedding_df['vocab'] = input_embedding_df['index'].str.extract(r'/c/en/(\\w+)/?')\n",
    "input_embedding_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4081, 128)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-1.7784444e-03,  7.0306961e-03,  2.2962685e-03, ...,\n",
       "        -2.3585208e-01, -3.1781605e-01, -4.9836105e-01],\n",
       "       [-1.7784444e-03,  7.0306961e-03,  2.2962685e-03, ...,\n",
       "        -2.3585208e-01, -3.1781605e-01, -4.9836105e-01],\n",
       "       [-1.7784444e-03,  7.0306961e-03,  2.2962685e-03, ...,\n",
       "        -2.3585208e-01, -3.1781605e-01, -4.9836105e-01],\n",
       "       ...,\n",
       "       [-3.5266747e-04,  1.3941947e-03,  4.5535257e-04, ...,\n",
       "        -4.6769723e-02, -6.3023269e-02, -9.8825537e-02],\n",
       "       [-3.5266747e-04,  1.3941947e-03,  4.5535257e-04, ...,\n",
       "        -4.6769723e-02, -6.3023269e-02, -9.8825537e-02],\n",
       "       [-3.5266747e-04,  1.3941947e-03,  4.5535257e-04, ...,\n",
       "        -4.6769723e-02, -6.3023269e-02, -9.8825537e-02]],\n",
       "      shape=(4081, 128), dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert retrofit ppimi word embedding into numpy matrix form\n",
    "input_embedding_matrix = input_embedding.to_numpy()\n",
    "print(input_embedding_matrix.shape)\n",
    "input_embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2016, 128)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.01316209,  0.00857995,  0.01150196, ..., -0.01960188,\n",
       "         0.01102599,  0.0168985 ],\n",
       "       [-0.02624886,  0.00065748,  0.00497713, ...,  0.00576143,\n",
       "        -0.01545058,  0.00221606],\n",
       "       [ 0.03140381, -0.02327314, -0.0265863 , ...,  0.00643379,\n",
       "         0.03365475, -0.03251279],\n",
       "       ...,\n",
       "       [ 0.01972965,  0.01448449, -0.02070585, ...,  0.0077508 ,\n",
       "        -0.02492797,  0.00269021],\n",
       "       [-0.01042436,  0.01346194, -0.04289725, ...,  0.01183029,\n",
       "         0.00020893, -0.01214224],\n",
       "       [ 0.01525993, -0.04257187, -0.01507291, ...,  0.00786374,\n",
       "        -0.01781924,  0.00746477]], shape=(2016, 128), dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get Embedding Weights of ALBERT model\n",
    "# embedding_layer = model.embeddings.word_embeddings # For AlbertModel object\n",
    "mobilebert_model = model._modules['mobilebert']\n",
    "embedding_layer = mobilebert_model.embeddings.word_embeddings\n",
    "\n",
    "# torch.no_grad() to avoid tracking gradients\n",
    "with torch.no_grad():\n",
    "    embedding_matrix = embedding_layer.weight.clone() # Clone to avoid modifying original\n",
    "\n",
    "default_embedding_matrix = embedding_matrix.cpu().numpy()\n",
    "print(default_embedding_matrix.shape)\n",
    "default_embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logic to modify default word embedding\n",
    "\n",
    "* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2016, 128])\n",
      "tensor([[-0.0132,  0.0086,  0.0115,  ..., -0.0196,  0.0110,  0.0169],\n",
      "        [-0.0003,  0.0012,  0.0005,  ..., -0.0473, -0.0490, -0.0898],\n",
      "        [ 0.0314, -0.0233, -0.0266,  ...,  0.0064,  0.0337, -0.0325],\n",
      "        ...,\n",
      "        [ 0.0197,  0.0145, -0.0207,  ...,  0.0078, -0.0249,  0.0027],\n",
      "        [-0.0104,  0.0135, -0.0429,  ...,  0.0118,  0.0002, -0.0121],\n",
      "        [ 0.0153, -0.0426, -0.0151,  ...,  0.0079, -0.0178,  0.0075]],\n",
      "       dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "modified_words = input_embedding_df['vocab'].to_list()\n",
    "\n",
    "def _tokenize(word:str):\n",
    "    # Handle case sensitivity based on the tokenizer\n",
    "    processed_word = word.lower() if tokenizer.do_lower_case else word\n",
    "\n",
    "    # Tokenize the word - it might split into subwords\n",
    "    tokens = tokenizer.tokenize(processed_word)\n",
    "    return tokens\n",
    "\n",
    "\n",
    "for idx, word in enumerate(modified_words):\n",
    "\n",
    "    tokens = _tokenize(word)\n",
    "\n",
    "    if len(tokens) == 1:\n",
    "\n",
    "        token = tokens[0]\n",
    "\n",
    "        embedding_idx = vocab[token]\n",
    "        new_embedding_array = input_embedding_matrix[idx]\n",
    "\n",
    "        default_embedding_matrix[embedding_idx] = new_embedding_array\n",
    "\n",
    "# Convert to PyTorch/TensorFlow tensor\n",
    "new_embedding_tensor = torch.tensor(default_embedding_matrix, dtype=torch.float16)\n",
    "\n",
    "print(new_embedding_tensor.shape)\n",
    "print(new_embedding_tensor)\n",
    "\n",
    "assert embedding_layer.weight.shape == new_embedding_tensor.shape, \\\n",
    "    f\"Shape mismatch: Model expects {embedding_layer.weight.shape}, got {new_embedding_tensor.shape}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the weights (ensure device placement is correct if using GPU)\n",
    "with torch.no_grad(): # Prevent tracking this operation in gradient history\n",
    "     embedding_layer.weight.copy_(new_embedding_tensor) # In-place copy is safer\n",
    "\n",
    "# Make sure the embedding layer is trainable (usually true by default after loading)\n",
    "embedding_layer.weight.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Finetuning - Sequence Classification task\n",
    "\n",
    "* Dataset: [`flax-sentence-embeddings/Gender_Bias_Evaluation_Set`](https://huggingface.co/datasets/flax-sentence-embeddings/Gender_Bias_Evaluation_Set)\n",
    "* Relatively small dataset for Sequence Classification task (1584 rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>base_sentence</th>\n",
       "      <th>occupation</th>\n",
       "      <th>stereotypical_gender</th>\n",
       "      <th>male_sentence</th>\n",
       "      <th>female_sentence</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>The developer argued with the designer because...</td>\n",
       "      <td>developer</td>\n",
       "      <td>male</td>\n",
       "      <td>He argued with the designer because he did not...</td>\n",
       "      <td>She argued with the designer because she did n...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>The developer argued with the designer because...</td>\n",
       "      <td>designer</td>\n",
       "      <td>female</td>\n",
       "      <td>The developer argued with him because his idea...</td>\n",
       "      <td>The developer argued with her because her idea...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>The mechanic gave the clerk a present because ...</td>\n",
       "      <td>mechanic</td>\n",
       "      <td>male</td>\n",
       "      <td>He gave the clerk a present because he won the...</td>\n",
       "      <td>She gave the clerk a present because she won t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>The mechanic gave the clerk a present because ...</td>\n",
       "      <td>clerk</td>\n",
       "      <td>female</td>\n",
       "      <td>The mechanic gave him a present because it was...</td>\n",
       "      <td>The mechanic gave her a present because it was...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>The mover said thank you to the housekeeper be...</td>\n",
       "      <td>mover</td>\n",
       "      <td>male</td>\n",
       "      <td>He said thank you to the housekeeper because h...</td>\n",
       "      <td>She said thank you to the housekeeper because ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                      base_sentence occupation  \\\n",
       "0           0  The developer argued with the designer because...  developer   \n",
       "1           1  The developer argued with the designer because...   designer   \n",
       "2           2  The mechanic gave the clerk a present because ...   mechanic   \n",
       "3           3  The mechanic gave the clerk a present because ...      clerk   \n",
       "4           4  The mover said thank you to the housekeeper be...      mover   \n",
       "\n",
       "  stereotypical_gender                                      male_sentence  \\\n",
       "0                 male  He argued with the designer because he did not...   \n",
       "1               female  The developer argued with him because his idea...   \n",
       "2                 male  He gave the clerk a present because he won the...   \n",
       "3               female  The mechanic gave him a present because it was...   \n",
       "4                 male  He said thank you to the housekeeper because h...   \n",
       "\n",
       "                                     female_sentence  labels  \n",
       "0  She argued with the designer because she did n...       1  \n",
       "1  The developer argued with her because her idea...       0  \n",
       "2  She gave the clerk a present because she won t...       1  \n",
       "3  The mechanic gave her a present because it was...       0  \n",
       "4  She said thank you to the housekeeper because ...       1  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"hf://datasets/flax-sentence-embeddings/Gender_Bias_Evaluation_Set/bias_evaluation.csv\")\n",
    "df['labels'] = df['stereotypical_gender'].apply(lambda x: 1 if x == \"male\" else 0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Unnamed: 0': 390,\n",
       " 'base_sentence': 'The salesperson contacted the tailor because she wanted to fix a suit.',\n",
       " 'occupation': 'salesperson',\n",
       " 'stereotypical_gender': 'male',\n",
       " 'male_sentence': 'He contacted the tailor because he wanted to fix a suit.',\n",
       " 'female_sentence': 'She contacted the tailor because she wanted to fix a suit.',\n",
       " 'labels': 1}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert dataset into Huggingface Dataset object with train-test split of 80:20\n",
    "datasets = Dataset.from_pandas(df).train_test_split(test_size=0.2)\n",
    "\n",
    "train_dataset = datasets[\"train\"]\n",
    "val_dataset = datasets[\"test\"]\n",
    "\n",
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1267/1267 [00:00<00:00, 3176.36 examples/s]\n",
      "Map: 100%|██████████| 317/317 [00:00<00:00, 8403.93 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the `base_sentence` column so that it can be used as input to finetune ALBERT\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"base_sentence\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_eval_dataset = val_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format the dataset for PyTorch - Remove columns not needed by the model\n",
    "cols_to_remove = [\"Unnamed: 0\", \"base_sentence\", \"occupation\", \"male_sentence\", \"female_sentence\", \"stereotypical_gender\"]\n",
    "tokenized_train_dataset = tokenized_train_dataset.remove_columns(cols_to_remove)\n",
    "tokenized_eval_dataset = tokenized_eval_dataset.remove_columns(cols_to_remove)\n",
    "\n",
    "# # Rename the 'stereotypical_gender' column to 'labels' (expected by Trainer)\n",
    "# tokenized_train_dataset = tokenized_train_dataset.rename_column(\"stereotypical_gender\", \"labels\")\n",
    "# tokenized_eval_dataset = tokenized_eval_dataset.rename_column(\"stereotypical_gender\", \"labels\")\n",
    "\n",
    "# Set format to PyTorch tensors\n",
    "tokenized_train_dataset.set_format(\"torch\")\n",
    "tokenized_eval_dataset.set_format(\"torch\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    # Logits are the raw output scores from the model, shape (batch_size, num_labels)\n",
    "    # Labels are the ground truth, shape (batch_size,)\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_454228/2816268275.py:20: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",             # Directory to save model checkpoints and logs\n",
    "    num_train_epochs=1,                 # Reduced for quick demonstration; use more epochs (e.g., 3-5) for real tasks\n",
    "    per_device_train_batch_size=8,      # Adjust based on your GPU memory\n",
    "    per_device_eval_batch_size=8,       # Adjust based on your GPU memory\n",
    "    warmup_steps=100,                   # Number of steps for linear warmup\n",
    "    weight_decay=0.01,                  # Regularization strength\n",
    "    logging_dir=\"./logs\",               # Directory for TensorBoard logs\n",
    "    logging_steps=50,                   # Log metrics every 50 steps\n",
    "    # evaluation_strategy=\"epoch\",        # Evaluate performance at the end of each epoch\n",
    "    # save_strategy=\"epoch\",              # Save model checkpoint at the end of each epoch\n",
    "    # load_best_model_at_end=True,        # Load the best model found during training at the end\n",
    "    metric_for_best_model=\"accuracy\",   # Metric used to determine the best model\n",
    "    greater_is_better=True,             # Accuracy should be maximized\n",
    "    report_to=\"tensorboard\",            # Report logs to TensorBoard (can add \"wandb\" etc.)\n",
    "    # push_to_hub=False,                # Set to True to push model to Hugging Face Hub\n",
    "    fp16=torch.cuda.is_available(),     # Use mixed precision training if CUDA is available\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                        # The model to train (potentially with custom embeddings)\n",
    "    args=training_args,                 # Training arguments defined above\n",
    "    train_dataset=tokenized_train_dataset, # Training dataset\n",
    "    eval_dataset=tokenized_eval_dataset,   # Evaluation dataset\n",
    "    tokenizer=tokenizer,                # Tokenizer used for data collation (handles padding dynamically if needed)\n",
    "    compute_metrics=compute_metrics,    # Function to compute evaluation metrics\n",
    "    # Optional: Data collator can optimize padding\n",
    "    # data_collator=DataCollatorWithPadding(tokenizer=tokenizer)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='159' max='159' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [159/159 00:30, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.650600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.590400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.420800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bernard/miniconda3/envs/cs4248_project/lib/python3.11/site-packages/transformers/configuration_utils.py:393: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'max_length': 128}\n",
      "  warnings.warn(\n",
      "/home/bernard/miniconda3/envs/cs4248_project/lib/python3.11/site-packages/transformers/configuration_utils.py:393: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'max_length': 128}\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** train metrics *****\n",
      "  epoch                    =        1.0\n",
      "  total_flos               =    74950GF\n",
      "  train_loss               =     0.8494\n",
      "  train_runtime            = 0:00:31.87\n",
      "  train_samples_per_second =      39.75\n",
      "  train_steps_per_second   =      4.988\n",
      "Evaluating the final model...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='40' max='40' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [40/40 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics: {'eval_loss': 0.2789522409439087, 'eval_accuracy': 0.8832807570977917, 'eval_runtime': 2.1046, 'eval_samples_per_second': 150.624, 'eval_steps_per_second': 19.006, 'epoch': 1.0}\n",
      "***** eval metrics *****\n",
      "  epoch                   =        1.0\n",
      "  eval_accuracy           =     0.8833\n",
      "  eval_loss               =      0.279\n",
      "  eval_runtime            = 0:00:02.10\n",
      "  eval_samples_per_second =    150.624\n",
      "  eval_steps_per_second   =     19.006\n"
     ]
    }
   ],
   "source": [
    "train_result = trainer.train()\n",
    "\n",
    "trainer.save_model()  # Saves the tokenizer too\n",
    "trainer.log_metrics(\"train\", train_result.metrics)\n",
    "trainer.save_metrics(\"train\", train_result.metrics)\n",
    "trainer.save_state()\n",
    "\n",
    "# 6. Evaluate the Final Model\n",
    "print(\"Evaluating the final model...\")\n",
    "eval_metrics = trainer.evaluate()\n",
    "print(f\"Evaluation Metrics: {eval_metrics}\")\n",
    "trainer.log_metrics(\"eval\", eval_metrics)\n",
    "trainer.save_metrics(\"eval\", eval_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2016, 128)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.01316017,  0.00858272,  0.01150468, ..., -0.01960675,\n",
       "         0.01102403,  0.01689079],\n",
       "       [ 0.00010355,  0.00037436, -0.00077101, ..., -0.04639971,\n",
       "        -0.0490229 , -0.08949148],\n",
       "       [ 0.03108884, -0.0236288 , -0.02622124, ...,  0.0061381 ,\n",
       "         0.0333262 , -0.03211844],\n",
       "       ...,\n",
       "       [ 0.0199623 ,  0.01472118, -0.02047166, ...,  0.00751895,\n",
       "        -0.02516552,  0.00245555],\n",
       "       [-0.01042134,  0.01345772, -0.04290605, ...,  0.01183271,\n",
       "         0.00020897, -0.01214551],\n",
       "       [ 0.01525817, -0.04257036, -0.01507506, ...,  0.00786556,\n",
       "        -0.01782154,  0.00746505]], shape=(2016, 128), dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Access the embedding layer again (use the same path as in Step 4)\n",
    "final_embedding_layer = mobilebert_model.embeddings.word_embeddings\n",
    "\n",
    "# Get the weights\n",
    "final_embeddings_tensor = final_embedding_layer.weight.data\n",
    "\n",
    "# Convert to NumPy if desired (and move to CPU if on GPU)\n",
    "final_embeddings_numpy = final_embeddings_tensor.cpu().numpy()\n",
    "print(final_embeddings_numpy.shape)\n",
    "final_embeddings_numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs4248_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
