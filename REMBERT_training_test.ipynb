{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from utils.formats import load_hdf\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from datasets import Dataset\n",
    "from transformers import PreTrainedModel, AutoModel, AutoTokenizer, DistilBertForSequenceClassification \n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers.configuration_utils import PretrainedConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Huggingface model - [REMBERT](https://huggingface.co/docs/transformers/model_doc/rembert)\n",
    "\n",
    "* Input Embedding Dimensionality cannot be too big. \n",
    "* Standard Flavours of BERT-based transformer models have input dim of 768. PPMi + Retrofitting takes too long to produce input embedding vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DistilBertConfig' object has no attribute 'embedding_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      7\u001b[39m vocab = tokenizer.get_vocab()\n\u001b[32m      8\u001b[39m vocab_size = \u001b[38;5;28mlen\u001b[39m(vocab)\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m embedding_dim = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43membedding_size\u001b[49m  \u001b[38;5;66;03m#hidden_size\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTokenizer Vocab Size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvocab_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mEmbedding Dimensionality: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00membedding_dim\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mVocab:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mvocab\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/cs4248_project/lib/python3.11/site-packages/transformers/configuration_utils.py:210\u001b[39m, in \u001b[36mPretrainedConfig.__getattribute__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m    208\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m key != \u001b[33m\"\u001b[39m\u001b[33mattribute_map\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__getattribute__\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mattribute_map\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    209\u001b[39m     key = \u001b[38;5;28msuper\u001b[39m().\u001b[34m__getattribute__\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mattribute_map\u001b[39m\u001b[33m\"\u001b[39m)[key]\n\u001b[32m--> \u001b[39m\u001b[32m210\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__getattribute__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mAttributeError\u001b[39m: 'DistilBertConfig' object has no attribute 'embedding_size'"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"distilbert-base-uncased\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = DistilBertForSequenceClassification.from_pretrained(MODEL_NAME) # Specify num_labels for your task\n",
    "# model = AlbertModel.from_pretrained(MODEL_NAME, torch_dtype=torch.float16) # Specify num_labels for your task\n",
    "\n",
    "vocab = tokenizer.get_vocab()\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = model.config.embedding_size  #hidden_size\n",
    "print(f\"Tokenizer Vocab Size: {vocab_size}\\nEmbedding Dimensionality: {embedding_dim}\")\n",
    "print(f\"Vocab:\\n{vocab}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'training': False,\n",
       " '_parameters': {},\n",
       " '_buffers': {},\n",
       " '_non_persistent_buffers_set': set(),\n",
       " '_backward_pre_hooks': OrderedDict(),\n",
       " '_backward_hooks': OrderedDict(),\n",
       " '_is_full_backward_hook': None,\n",
       " '_forward_hooks': OrderedDict(),\n",
       " '_forward_hooks_with_kwargs': OrderedDict(),\n",
       " '_forward_hooks_always_called': OrderedDict(),\n",
       " '_forward_pre_hooks': OrderedDict(),\n",
       " '_forward_pre_hooks_with_kwargs': OrderedDict(),\n",
       " '_state_dict_hooks': OrderedDict(),\n",
       " '_state_dict_pre_hooks': OrderedDict(),\n",
       " '_load_state_dict_pre_hooks': OrderedDict(),\n",
       " '_load_state_dict_post_hooks': OrderedDict(),\n",
       " '_modules': {'distilbert': DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): DistilBertSdpaAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  ),\n",
       "  'pre_classifier': Linear(in_features=768, out_features=768, bias=True),\n",
       "  'classifier': Linear(in_features=768, out_features=2, bias=True),\n",
       "  'dropout': Dropout(p=0.2, inplace=False)},\n",
       " 'config': DistilBertConfig {\n",
       "   \"_attn_implementation_autoset\": true,\n",
       "   \"activation\": \"gelu\",\n",
       "   \"architectures\": [\n",
       "     \"DistilBertForMaskedLM\"\n",
       "   ],\n",
       "   \"attention_dropout\": 0.1,\n",
       "   \"dim\": 768,\n",
       "   \"dropout\": 0.1,\n",
       "   \"hidden_dim\": 3072,\n",
       "   \"initializer_range\": 0.02,\n",
       "   \"max_position_embeddings\": 512,\n",
       "   \"model_type\": \"distilbert\",\n",
       "   \"n_heads\": 12,\n",
       "   \"n_layers\": 6,\n",
       "   \"pad_token_id\": 0,\n",
       "   \"qa_dropout\": 0.1,\n",
       "   \"seq_classif_dropout\": 0.2,\n",
       "   \"sinusoidal_pos_embds\": false,\n",
       "   \"tie_weights_\": true,\n",
       "   \"torch_dtype\": \"float32\",\n",
       "   \"transformers_version\": \"4.51.0\",\n",
       "   \"vocab_size\": 30522\n",
       " },\n",
       " 'loss_type': 'ForSequenceClassification',\n",
       " 'name_or_path': 'distilbert-base-uncased',\n",
       " 'warnings_issued': {},\n",
       " 'generation_config': None,\n",
       " '_keep_in_fp32_modules': None,\n",
       " '_no_split_modules': [],\n",
       " 'num_labels': 2,\n",
       " '_pp_plan': None,\n",
       " '_tp_plan': {},\n",
       " '_is_hf_initialized': True}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Retrofitted PPMI word embeddings for ALBERT (dim=128)\n",
    "\n",
    "* Since index of input word embedding matrix after retrofitting can contain multiple words due to edge connections, data cleaning is required to process the index such that one word remains (e.g. `/c/en/president/n/wn/person` --> `president`)\n",
    "* This step required to match ALBERT tokenizer's vocab so that the corresponding input word embedding can be identified and modified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>119</th>\n",
       "      <th>120</th>\n",
       "      <th>121</th>\n",
       "      <th>122</th>\n",
       "      <th>123</th>\n",
       "      <th>124</th>\n",
       "      <th>125</th>\n",
       "      <th>126</th>\n",
       "      <th>127</th>\n",
       "      <th>vocab</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/c/en/chair_meeting</td>\n",
       "      <td>-0.001778</td>\n",
       "      <td>0.007031</td>\n",
       "      <td>0.002296</td>\n",
       "      <td>0.00364</td>\n",
       "      <td>0.004209</td>\n",
       "      <td>-0.007875</td>\n",
       "      <td>0.001394</td>\n",
       "      <td>0.006352</td>\n",
       "      <td>0.004029</td>\n",
       "      <td>...</td>\n",
       "      <td>0.146947</td>\n",
       "      <td>0.198373</td>\n",
       "      <td>-0.264603</td>\n",
       "      <td>0.27145</td>\n",
       "      <td>-0.278929</td>\n",
       "      <td>-0.268215</td>\n",
       "      <td>-0.235852</td>\n",
       "      <td>-0.317816</td>\n",
       "      <td>-0.498361</td>\n",
       "      <td>chair_meeting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/c/en/chairperson</td>\n",
       "      <td>-0.001778</td>\n",
       "      <td>0.007031</td>\n",
       "      <td>0.002296</td>\n",
       "      <td>0.00364</td>\n",
       "      <td>0.004209</td>\n",
       "      <td>-0.007875</td>\n",
       "      <td>0.001394</td>\n",
       "      <td>0.006352</td>\n",
       "      <td>0.004029</td>\n",
       "      <td>...</td>\n",
       "      <td>0.146947</td>\n",
       "      <td>0.198373</td>\n",
       "      <td>-0.264603</td>\n",
       "      <td>0.27145</td>\n",
       "      <td>-0.278929</td>\n",
       "      <td>-0.268215</td>\n",
       "      <td>-0.235852</td>\n",
       "      <td>-0.317816</td>\n",
       "      <td>-0.498361</td>\n",
       "      <td>chairperson</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/c/en/chair</td>\n",
       "      <td>-0.001778</td>\n",
       "      <td>0.007031</td>\n",
       "      <td>0.002296</td>\n",
       "      <td>0.00364</td>\n",
       "      <td>0.004209</td>\n",
       "      <td>-0.007875</td>\n",
       "      <td>0.001394</td>\n",
       "      <td>0.006352</td>\n",
       "      <td>0.004029</td>\n",
       "      <td>...</td>\n",
       "      <td>0.146947</td>\n",
       "      <td>0.198373</td>\n",
       "      <td>-0.264603</td>\n",
       "      <td>0.27145</td>\n",
       "      <td>-0.278929</td>\n",
       "      <td>-0.268215</td>\n",
       "      <td>-0.235852</td>\n",
       "      <td>-0.317816</td>\n",
       "      <td>-0.498361</td>\n",
       "      <td>chair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/c/en/chairperson/n</td>\n",
       "      <td>-0.001778</td>\n",
       "      <td>0.007031</td>\n",
       "      <td>0.002296</td>\n",
       "      <td>0.00364</td>\n",
       "      <td>0.004209</td>\n",
       "      <td>-0.007875</td>\n",
       "      <td>0.001394</td>\n",
       "      <td>0.006352</td>\n",
       "      <td>0.004029</td>\n",
       "      <td>...</td>\n",
       "      <td>0.146947</td>\n",
       "      <td>0.198373</td>\n",
       "      <td>-0.264603</td>\n",
       "      <td>0.27145</td>\n",
       "      <td>-0.278929</td>\n",
       "      <td>-0.268215</td>\n",
       "      <td>-0.235852</td>\n",
       "      <td>-0.317816</td>\n",
       "      <td>-0.498361</td>\n",
       "      <td>chairperson</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/c/en/president/n/wn/person</td>\n",
       "      <td>-0.001778</td>\n",
       "      <td>0.007031</td>\n",
       "      <td>0.002296</td>\n",
       "      <td>0.00364</td>\n",
       "      <td>0.004209</td>\n",
       "      <td>-0.007875</td>\n",
       "      <td>0.001394</td>\n",
       "      <td>0.006352</td>\n",
       "      <td>0.004029</td>\n",
       "      <td>...</td>\n",
       "      <td>0.146947</td>\n",
       "      <td>0.198373</td>\n",
       "      <td>-0.264603</td>\n",
       "      <td>0.27145</td>\n",
       "      <td>-0.278929</td>\n",
       "      <td>-0.268215</td>\n",
       "      <td>-0.235852</td>\n",
       "      <td>-0.317816</td>\n",
       "      <td>-0.498361</td>\n",
       "      <td>president</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 130 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         index         0         1         2        3  \\\n",
       "0          /c/en/chair_meeting -0.001778  0.007031  0.002296  0.00364   \n",
       "1            /c/en/chairperson -0.001778  0.007031  0.002296  0.00364   \n",
       "2                  /c/en/chair -0.001778  0.007031  0.002296  0.00364   \n",
       "3          /c/en/chairperson/n -0.001778  0.007031  0.002296  0.00364   \n",
       "4  /c/en/president/n/wn/person -0.001778  0.007031  0.002296  0.00364   \n",
       "\n",
       "          4         5         6         7         8  ...       119       120  \\\n",
       "0  0.004209 -0.007875  0.001394  0.006352  0.004029  ...  0.146947  0.198373   \n",
       "1  0.004209 -0.007875  0.001394  0.006352  0.004029  ...  0.146947  0.198373   \n",
       "2  0.004209 -0.007875  0.001394  0.006352  0.004029  ...  0.146947  0.198373   \n",
       "3  0.004209 -0.007875  0.001394  0.006352  0.004029  ...  0.146947  0.198373   \n",
       "4  0.004209 -0.007875  0.001394  0.006352  0.004029  ...  0.146947  0.198373   \n",
       "\n",
       "        121      122       123       124       125       126       127  \\\n",
       "0 -0.264603  0.27145 -0.278929 -0.268215 -0.235852 -0.317816 -0.498361   \n",
       "1 -0.264603  0.27145 -0.278929 -0.268215 -0.235852 -0.317816 -0.498361   \n",
       "2 -0.264603  0.27145 -0.278929 -0.268215 -0.235852 -0.317816 -0.498361   \n",
       "3 -0.264603  0.27145 -0.278929 -0.268215 -0.235852 -0.317816 -0.498361   \n",
       "4 -0.264603  0.27145 -0.278929 -0.268215 -0.235852 -0.317816 -0.498361   \n",
       "\n",
       "           vocab  \n",
       "0  chair_meeting  \n",
       "1    chairperson  \n",
       "2          chair  \n",
       "3    chairperson  \n",
       "4      president  \n",
       "\n",
       "[5 rows x 130 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_embedding = load_hdf(\"data/conceptnet_api/retrofit/retrofitted-albert-128\")\n",
    "input_embedding_df = input_embedding.reset_index()\n",
    "input_embedding_df['vocab'] = input_embedding_df['index'].str.extract(r'/c/en/(\\w+)/?')\n",
    "input_embedding_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4081, 128)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-1.7784444e-03,  7.0306961e-03,  2.2962685e-03, ...,\n",
       "        -2.3585208e-01, -3.1781605e-01, -4.9836105e-01],\n",
       "       [-1.7784444e-03,  7.0306961e-03,  2.2962685e-03, ...,\n",
       "        -2.3585208e-01, -3.1781605e-01, -4.9836105e-01],\n",
       "       [-1.7784444e-03,  7.0306961e-03,  2.2962685e-03, ...,\n",
       "        -2.3585208e-01, -3.1781605e-01, -4.9836105e-01],\n",
       "       ...,\n",
       "       [-3.5266747e-04,  1.3941947e-03,  4.5535257e-04, ...,\n",
       "        -4.6769723e-02, -6.3023269e-02, -9.8825537e-02],\n",
       "       [-3.5266747e-04,  1.3941947e-03,  4.5535257e-04, ...,\n",
       "        -4.6769723e-02, -6.3023269e-02, -9.8825537e-02],\n",
       "       [-3.5266747e-04,  1.3941947e-03,  4.5535257e-04, ...,\n",
       "        -4.6769723e-02, -6.3023269e-02, -9.8825537e-02]],\n",
       "      shape=(4081, 128), dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert retrofit ppimi word embedding into numpy matrix form\n",
    "input_embedding_matrix = input_embedding.to_numpy()\n",
    "print(input_embedding_matrix.shape)\n",
    "input_embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30000, 128)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.05101773, -0.05638105, -0.08745944, ...,  0.10348055,\n",
       "        -0.1064435 , -0.06387638],\n",
       "       [ 0.08651973,  0.02260554, -0.03166365, ..., -0.06117148,\n",
       "        -0.05314829, -0.0543424 ],\n",
       "       [-0.01066898,  0.01375878, -0.02094011, ...,  0.03288412,\n",
       "        -0.01413923,  0.02857986],\n",
       "       ...,\n",
       "       [ 0.02059551,  0.03651065, -0.09545734, ..., -0.0247529 ,\n",
       "         0.13839178, -0.05422436],\n",
       "       [-0.11926416, -0.11318762,  0.05813185, ..., -0.07097802,\n",
       "         0.08779413,  0.22770554],\n",
       "       [ 0.07093989, -0.11180934, -0.01000072, ...,  0.08569918,\n",
       "         0.1817395 , -0.03343155]], shape=(30000, 128), dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get Embedding Weights of ALBERT model\n",
    "# embedding_layer = model.embeddings.word_embeddings # For AlbertModel object\n",
    "albert_model = model._modules['albert']\n",
    "embedding_layer = albert_model.embeddings.word_embeddings\n",
    "\n",
    "# torch.no_grad() to avoid tracking gradients\n",
    "with torch.no_grad():\n",
    "    embedding_matrix = embedding_layer.weight.clone() # Clone to avoid modifying original\n",
    "\n",
    "default_embedding_matrix = embedding_matrix.cpu().numpy()\n",
    "print(default_embedding_matrix.shape)\n",
    "default_embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logic to modify default word embedding\n",
    "\n",
    "* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30000, 128])\n",
      "tensor([[-0.0510, -0.0564, -0.0875,  ...,  0.1035, -0.1064, -0.0639],\n",
      "        [ 0.0865,  0.0226, -0.0317,  ..., -0.0612, -0.0532, -0.0544],\n",
      "        [-0.0107,  0.0138, -0.0209,  ...,  0.0329, -0.0141,  0.0286],\n",
      "        ...,\n",
      "        [ 0.0206,  0.0365, -0.0955,  ..., -0.0247,  0.1384, -0.0542],\n",
      "        [-0.1193, -0.1132,  0.0581,  ..., -0.0710,  0.0878,  0.2277],\n",
      "        [ 0.0709, -0.1118, -0.0100,  ...,  0.0857,  0.1818, -0.0334]],\n",
      "       dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "modified_words = input_embedding_df['vocab'].to_list()\n",
    "\n",
    "def _tokenize(word:str):\n",
    "    # Handle case sensitivity based on the tokenizer\n",
    "    processed_word = word.lower() if tokenizer.do_lower_case else word\n",
    "\n",
    "    # Tokenize the word - it might split into subwords\n",
    "    tokens = tokenizer.tokenize(processed_word)\n",
    "    return tokens\n",
    "\n",
    "\n",
    "for idx, word in enumerate(modified_words):\n",
    "\n",
    "    tokens = _tokenize(word)\n",
    "\n",
    "    if len(tokens) == 1:\n",
    "\n",
    "        token = tokens[0]\n",
    "\n",
    "        embedding_idx = vocab[token]\n",
    "        new_embedding_array = input_embedding_matrix[idx]\n",
    "\n",
    "        default_embedding_matrix[embedding_idx] = new_embedding_array\n",
    "\n",
    "# Convert to PyTorch/TensorFlow tensor\n",
    "new_embedding_tensor = torch.tensor(default_embedding_matrix, dtype=torch.float16)\n",
    "\n",
    "print(new_embedding_tensor.shape)\n",
    "print(new_embedding_tensor)\n",
    "\n",
    "assert embedding_layer.weight.shape == new_embedding_tensor.shape, \\\n",
    "    f\"Shape mismatch: Model expects {embedding_layer.weight.shape}, got {new_embedding_tensor.shape}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the weights (ensure device placement is correct if using GPU)\n",
    "with torch.no_grad(): # Prevent tracking this operation in gradient history\n",
    "     embedding_layer.weight.copy_(new_embedding_tensor) # In-place copy is safer\n",
    "\n",
    "# Make sure the embedding layer is trainable (usually true by default after loading)\n",
    "embedding_layer.weight.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Finetuning - Sequence Classification task\n",
    "\n",
    "* Dataset: [`flax-sentence-embeddings/Gender_Bias_Evaluation_Set`](https://huggingface.co/datasets/flax-sentence-embeddings/Gender_Bias_Evaluation_Set)\n",
    "* Relatively small dataset for Sequence Classification task (1584 rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>base_sentence</th>\n",
       "      <th>occupation</th>\n",
       "      <th>stereotypical_gender</th>\n",
       "      <th>male_sentence</th>\n",
       "      <th>female_sentence</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>The developer argued with the designer because...</td>\n",
       "      <td>developer</td>\n",
       "      <td>male</td>\n",
       "      <td>He argued with the designer because he did not...</td>\n",
       "      <td>She argued with the designer because she did n...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>The developer argued with the designer because...</td>\n",
       "      <td>designer</td>\n",
       "      <td>female</td>\n",
       "      <td>The developer argued with him because his idea...</td>\n",
       "      <td>The developer argued with her because her idea...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>The mechanic gave the clerk a present because ...</td>\n",
       "      <td>mechanic</td>\n",
       "      <td>male</td>\n",
       "      <td>He gave the clerk a present because he won the...</td>\n",
       "      <td>She gave the clerk a present because she won t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>The mechanic gave the clerk a present because ...</td>\n",
       "      <td>clerk</td>\n",
       "      <td>female</td>\n",
       "      <td>The mechanic gave him a present because it was...</td>\n",
       "      <td>The mechanic gave her a present because it was...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>The mover said thank you to the housekeeper be...</td>\n",
       "      <td>mover</td>\n",
       "      <td>male</td>\n",
       "      <td>He said thank you to the housekeeper because h...</td>\n",
       "      <td>She said thank you to the housekeeper because ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                      base_sentence occupation  \\\n",
       "0           0  The developer argued with the designer because...  developer   \n",
       "1           1  The developer argued with the designer because...   designer   \n",
       "2           2  The mechanic gave the clerk a present because ...   mechanic   \n",
       "3           3  The mechanic gave the clerk a present because ...      clerk   \n",
       "4           4  The mover said thank you to the housekeeper be...      mover   \n",
       "\n",
       "  stereotypical_gender                                      male_sentence  \\\n",
       "0                 male  He argued with the designer because he did not...   \n",
       "1               female  The developer argued with him because his idea...   \n",
       "2                 male  He gave the clerk a present because he won the...   \n",
       "3               female  The mechanic gave him a present because it was...   \n",
       "4                 male  He said thank you to the housekeeper because h...   \n",
       "\n",
       "                                     female_sentence  labels  \n",
       "0  She argued with the designer because she did n...       1  \n",
       "1  The developer argued with her because her idea...       0  \n",
       "2  She gave the clerk a present because she won t...       1  \n",
       "3  The mechanic gave her a present because it was...       0  \n",
       "4  She said thank you to the housekeeper because ...       1  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"hf://datasets/flax-sentence-embeddings/Gender_Bias_Evaluation_Set/bias_evaluation.csv\")\n",
    "df['labels'] = df['stereotypical_gender'].apply(lambda x: 1 if x == \"male\" else 0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Unnamed: 0': 1022,\n",
       " 'base_sentence': \"The developer needed the clerk 's help with paperwork and asked him.\",\n",
       " 'occupation': 'clerk',\n",
       " 'stereotypical_gender': 'female',\n",
       " 'male_sentence': \"The developer needed him 's help with paperwork and asked him.\",\n",
       " 'female_sentence': \"The developer needed her 's help with paperwork and asked him.\",\n",
       " 'labels': 0}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert dataset into Huggingface Dataset object with train-test split of 80:20\n",
    "datasets = Dataset.from_pandas(df).train_test_split(test_size=0.2)\n",
    "\n",
    "train_dataset = datasets[\"train\"]\n",
    "val_dataset = datasets[\"test\"]\n",
    "\n",
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1267/1267 [00:00<00:00, 4439.69 examples/s]\n",
      "Map: 100%|██████████| 317/317 [00:00<00:00, 8325.68 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the `base_sentence` column so that it can be used as input to finetune ALBERT\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"base_sentence\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_eval_dataset = val_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format the dataset for PyTorch - Remove columns not needed by the model\n",
    "cols_to_remove = [\"Unnamed: 0\", \"base_sentence\", \"occupation\", \"male_sentence\", \"female_sentence\", \"stereotypical_gender\"]\n",
    "tokenized_train_dataset = tokenized_train_dataset.remove_columns(cols_to_remove)\n",
    "tokenized_eval_dataset = tokenized_eval_dataset.remove_columns(cols_to_remove)\n",
    "\n",
    "# # Rename the 'stereotypical_gender' column to 'labels' (expected by Trainer)\n",
    "# tokenized_train_dataset = tokenized_train_dataset.rename_column(\"stereotypical_gender\", \"labels\")\n",
    "# tokenized_eval_dataset = tokenized_eval_dataset.rename_column(\"stereotypical_gender\", \"labels\")\n",
    "\n",
    "# Set format to PyTorch tensors\n",
    "tokenized_train_dataset.set_format(\"torch\")\n",
    "tokenized_eval_dataset.set_format(\"torch\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    # Logits are the raw output scores from the model, shape (batch_size, num_labels)\n",
    "    # Labels are the ground truth, shape (batch_size,)\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_391968/2816268275.py:20: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",             # Directory to save model checkpoints and logs\n",
    "    num_train_epochs=1,                 # Reduced for quick demonstration; use more epochs (e.g., 3-5) for real tasks\n",
    "    per_device_train_batch_size=8,      # Adjust based on your GPU memory\n",
    "    per_device_eval_batch_size=8,       # Adjust based on your GPU memory\n",
    "    warmup_steps=100,                   # Number of steps for linear warmup\n",
    "    weight_decay=0.01,                  # Regularization strength\n",
    "    logging_dir=\"./logs\",               # Directory for TensorBoard logs\n",
    "    logging_steps=50,                   # Log metrics every 50 steps\n",
    "    # evaluation_strategy=\"epoch\",        # Evaluate performance at the end of each epoch\n",
    "    # save_strategy=\"epoch\",              # Save model checkpoint at the end of each epoch\n",
    "    # load_best_model_at_end=True,        # Load the best model found during training at the end\n",
    "    metric_for_best_model=\"accuracy\",   # Metric used to determine the best model\n",
    "    greater_is_better=True,             # Accuracy should be maximized\n",
    "    report_to=\"tensorboard\",            # Report logs to TensorBoard (can add \"wandb\" etc.)\n",
    "    # push_to_hub=False,                # Set to True to push model to Hugging Face Hub\n",
    "    fp16=torch.cuda.is_available(),     # Use mixed precision training if CUDA is available\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                        # The model to train (potentially with custom embeddings)\n",
    "    args=training_args,                 # Training arguments defined above\n",
    "    train_dataset=tokenized_train_dataset, # Training dataset\n",
    "    eval_dataset=tokenized_eval_dataset,   # Evaluation dataset\n",
    "    tokenizer=tokenizer,                # Tokenizer used for data collation (handles padding dynamically if needed)\n",
    "    compute_metrics=compute_metrics,    # Function to compute evaluation metrics\n",
    "    # Optional: Data collator can optimize padding\n",
    "    # data_collator=DataCollatorWithPadding(tokenizer=tokenizer)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='159' max='159' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [159/159 00:23, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.708500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.166500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.021000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** train metrics *****\n",
      "  epoch                    =        1.0\n",
      "  total_flos               =    28199GF\n",
      "  train_loss               =     0.2818\n",
      "  train_runtime            = 0:00:24.58\n",
      "  train_samples_per_second =     51.531\n",
      "  train_steps_per_second   =      6.467\n",
      "Evaluating the final model...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='40' max='40' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [40/40 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics: {'eval_loss': 0.00017151095380540937, 'eval_accuracy': 1.0, 'eval_runtime': 1.7117, 'eval_samples_per_second': 185.193, 'eval_steps_per_second': 23.368, 'epoch': 1.0}\n",
      "***** eval metrics *****\n",
      "  epoch                   =        1.0\n",
      "  eval_accuracy           =        1.0\n",
      "  eval_loss               =     0.0002\n",
      "  eval_runtime            = 0:00:01.71\n",
      "  eval_samples_per_second =    185.193\n",
      "  eval_steps_per_second   =     23.368\n"
     ]
    }
   ],
   "source": [
    "train_result = trainer.train()\n",
    "\n",
    "trainer.save_model()  # Saves the tokenizer too\n",
    "trainer.log_metrics(\"train\", train_result.metrics)\n",
    "trainer.save_metrics(\"train\", train_result.metrics)\n",
    "trainer.save_state()\n",
    "\n",
    "# 6. Evaluate the Final Model\n",
    "print(\"Evaluating the final model...\")\n",
    "eval_metrics = trainer.evaluate()\n",
    "print(f\"Evaluation Metrics: {eval_metrics}\")\n",
    "trainer.log_metrics(\"eval\", eval_metrics)\n",
    "trainer.save_metrics(\"eval\", eval_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30000, 128)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.05102333, -0.05636369, -0.08745994, ...,  0.10345046,\n",
       "        -0.10644104, -0.06390105],\n",
       "       [ 0.08654442,  0.02259737, -0.03167587, ..., -0.06115475,\n",
       "        -0.05315949, -0.05434967],\n",
       "       [-0.01059559,  0.01394353, -0.02050941, ...,  0.03291259,\n",
       "        -0.0143677 ,  0.02812883],\n",
       "       ...,\n",
       "       [ 0.02059854,  0.03649751, -0.09545512, ..., -0.02474876,\n",
       "         0.13842222, -0.0542276 ],\n",
       "       [-0.11925773, -0.11315463,  0.05813357, ..., -0.070981  ,\n",
       "         0.08776498,  0.22765203],\n",
       "       [ 0.07091997, -0.111812  , -0.01000174, ...,  0.08569005,\n",
       "         0.18175554, -0.03341537]], shape=(30000, 128), dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Access the embedding layer again (use the same path as in Step 4)\n",
    "final_embedding_layer = albert_model.embeddings.word_embeddings\n",
    "\n",
    "# Get the weights\n",
    "final_embeddings_tensor = final_embedding_layer.weight.data\n",
    "\n",
    "# Convert to NumPy if desired (and move to CPU if on GPU)\n",
    "final_embeddings_numpy = final_embeddings_tensor.cpu().numpy()\n",
    "print(final_embeddings_numpy.shape)\n",
    "final_embeddings_numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000 512\n"
     ]
    }
   ],
   "source": [
    "from transformers import MobileBertConfig, MobileBertModel\n",
    "\n",
    "# Initializing a MobileBERT configuration\n",
    "configuration = MobileBertConfig()\n",
    "\n",
    "# Initializing a model (with random weights) from the configuration above\n",
    "model = MobileBertModel(configuration)\n",
    "\n",
    "# Accessing the model configuration\n",
    "configuration = model.config\n",
    "\n",
    "vocab = tokenizer.get_vocab()\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = model.config.hidden_size\n",
    "print(vocab_size, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250680 256\n"
     ]
    }
   ],
   "source": [
    "from transformers import RemBertModel, RemBertConfig\n",
    "\n",
    "# Initializing a RemBERT rembert style configuration\n",
    "configuration = RemBertConfig()\n",
    "\n",
    "# Initializing a model from the rembert style configuration\n",
    "model = RemBertModel(configuration)\n",
    "\n",
    "vocab = tokenizer.get_vocab()\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = model.config.input_embedding_size \n",
    "print(vocab_size, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30522, 768])\n"
     ]
    }
   ],
   "source": [
    "# Example for BERT-like models (check your specific model structure)\n",
    "embedding_layer = model.bert.embeddings.word_embeddings # Adjust path as needed (e.g., model.roberta...)\n",
    "\n",
    "# Check dimensions match\n",
    "print(embedding_layer.weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Dataset with Pre-Computed Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingDataset(Dataset):\n",
    "    def __init__(self, embeddings, labels):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embeddings: Pre-computed embeddings (numpy array or torch tensor)\n",
    "                       Shape: (num_samples, seq_length, embedding_dim)\n",
    "            labels: Corresponding labels\n",
    "        \"\"\"\n",
    "        self.embeddings = torch.tensor(embeddings, dtype=torch.float)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_embeddings': self.embeddings[idx],\n",
    "            'labels': self.labels[idx]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 593,  595,  590, ..., 2518, 2514, 2516], shape=(4081,))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def convert_labels_to_int(labels):\n",
    "    \"\"\"\n",
    "    Convert various label formats to integer labels\n",
    "    \n",
    "    Args:\n",
    "        labels: Could be strings, one-hot, etc.\n",
    "    \n",
    "    Returns:\n",
    "        numpy array of integer labels\n",
    "    \"\"\"\n",
    "    if isinstance(labels[0], str):\n",
    "        # String labels to integers\n",
    "        unique_labels = sorted(set(labels))\n",
    "        label_to_int = {label: i for i, label in enumerate(unique_labels)}\n",
    "        return np.array([label_to_int[label] for label in labels])\n",
    "    elif len(labels.shape) > 1 and labels.shape[1] > 1:\n",
    "        # One-hot to integers\n",
    "        return np.argmax(labels, axis=1)\n",
    "    else:\n",
    "        # Already integers or binary\n",
    "        return labels.astype(int)\n",
    "    \n",
    "labels = convert_labels_to_int(input_embedding.index.to_list())\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingDataset(Dataset):\n",
    "    def __init__(self, embeddings, labels):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embeddings: Pre-computed embeddings (numpy array or torch tensor)\n",
    "                       Shape: (num_samples, seq_length, embedding_dim)\n",
    "            labels: Corresponding labels\n",
    "        \"\"\"\n",
    "        self.embeddings = torch.tensor(embeddings, dtype=torch.float)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_embeddings': self.embeddings[idx],\n",
    "            'labels': self.labels[idx]\n",
    "        }\n",
    "    \n",
    "dataset = EmbeddingDataset(embedding_array, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingConfig(PretrainedConfig):\n",
    "    def __init__(self, embedding_dim=300, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "class EmbeddingModel(PreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.transformer = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "        self.classifier = nn.Linear(config.embedding_dim, config.num_labels)\n",
    "        \n",
    "    def forward(self, input_embeddings, attention_mask=None, labels=None):\n",
    "        # Bypass the embedding layer and use pre-computed embeddings\n",
    "        outputs = self.transformer(\n",
    "            inputs_embeds=input_embeddings,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        \n",
    "        # Use [CLS] token for classification\n",
    "        logits = self.classifier(outputs.last_hidden_state[:, 0, :])\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fn = nn.CrossEntropyLoss()\n",
    "            loss = loss_fn(logits.view(-1, self.config.num_labels), labels.view(-1))\n",
    "            \n",
    "        return {\n",
    "            'loss': loss,\n",
    "            'logits': logits,\n",
    "            'hidden_states': outputs.hidden_states\n",
    "        }\n",
    "\n",
    "# Initialize model\n",
    "config = EmbeddingConfig(embedding_dim=300, num_labels=4081)\n",
    "model = EmbeddingModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     12\u001b[39m inputs = batch[\u001b[33m'\u001b[39m\u001b[33minput_embeddings\u001b[39m\u001b[33m'\u001b[39m].to(device)\n\u001b[32m     13\u001b[39m labels = batch[\u001b[33m'\u001b[39m\u001b[33mlabels\u001b[39m\u001b[33m'\u001b[39m].to(device)\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m loss = outputs[\u001b[33m'\u001b[39m\u001b[33mloss\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     17\u001b[39m loss.backward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/cs4248_project/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/cs4248_project/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36mEmbeddingModel.forward\u001b[39m\u001b[34m(self, input_embeddings, attention_mask, labels)\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_embeddings, attention_mask=\u001b[38;5;28;01mNone\u001b[39;00m, labels=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m     13\u001b[39m     \u001b[38;5;66;03m# Bypass the embedding layer and use pre-computed embeddings\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m     \u001b[38;5;66;03m# Use [CLS] token for classification\u001b[39;00m\n\u001b[32m     20\u001b[39m     logits = \u001b[38;5;28mself\u001b[39m.classifier(outputs.last_hidden_state[:, \u001b[32m0\u001b[39m, :])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/cs4248_project/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/cs4248_project/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/cs4248_project/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:1064\u001b[39m, in \u001b[36mBertModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m   1061\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1062\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mYou have to specify either input_ids or inputs_embeds\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1064\u001b[39m batch_size, seq_length = input_shape\n\u001b[32m   1065\u001b[39m device = input_ids.device \u001b[38;5;28;01mif\u001b[39;00m input_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m inputs_embeds.device\n\u001b[32m   1067\u001b[39m \u001b[38;5;66;03m# past_key_values_length\u001b[39;00m\n",
      "\u001b[31mValueError\u001b[39m: not enough values to unpack (expected 2, got 1)"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "train_loader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(3):\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        inputs = batch['input_embeddings'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        outputs = model(input_embeddings=inputs, labels=labels)\n",
    "        loss = outputs['loss']\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print(f\"Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingTrainer(Trainer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "    \n",
    "    def _prepare_inputs(self, inputs):\n",
    "        prepared = {}\n",
    "        for k, v in inputs.items():\n",
    "            if k == 'input_embeddings':\n",
    "                prepared['inputs_embeds'] = v.to(self.args.device)\n",
    "            elif isinstance(v, torch.Tensor):\n",
    "                prepared[k] = v.to(self.args.device)\n",
    "            else:\n",
    "                prepared[k] = v\n",
    "        return prepared\n",
    "\n",
    "# Usage with TrainingArguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./embedding_results\",\n",
    "    per_device_train_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-5,\n",
    ")\n",
    "\n",
    "trainer = EmbeddingTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    ")\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs4248_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
