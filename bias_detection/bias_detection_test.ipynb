{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "import csv\n",
    "import re \n",
    "import sys\n",
    "import os\n",
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load winobias data\n",
    "def load_winobias_data(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    sentences = [clean_winobias_text(line.strip()) for line in lines]\n",
    "    return sentences\n",
    "\n",
    "def clean_winobias_text(text):\n",
    "    # Remove line numbers (e.g., \"1. \", \"2. \")\n",
    "    text = re.sub(r'^\\d+\\s*', '', text)\n",
    "    # Remove brackets (e.g., \"[The nurse]\" â†’ \"The nurse\")\n",
    "    text = re.sub(r'\\[(.*?)\\]', lambda x: x.group(1), text)\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The developer argued with the designer because he did not like the design.', 'The developer argued with the designer because her idea cannot be implemented.', 'The mechanic gave the clerk a present because he won the lottery.', 'The mechanic gave the clerk a present because it was her birthday.', 'The mover said thank you to the housekeeper because he is grateful.']\n",
      "['The developer argued with the designer because she did not like the design.', 'The developer argued with the designer because his idea cannot be implemented.', 'The mechanic gave the clerk a present because she won the lottery.', 'The mechanic gave the clerk a present because it was his birthday.', 'The mover said thank you to the housekeeper because she is grateful.']\n"
     ]
    }
   ],
   "source": [
    "biased = load_winobias_data('winobias_data/pro_stereotyped_type1.txt.dev')\n",
    "print(biased[:5])\n",
    "\n",
    "anti_biased = load_winobias_data('winobias_data/anti_stereotyped_type1.txt.dev')\n",
    "print(anti_biased[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  label\n",
      "0  The salesperson chatted with the cleaner becau...      0\n",
      "1  The cook is always teaching the assistant new ...      0\n",
      "2  The mover moved the desk for the cleaner becau...      0\n",
      "3  The salesperson sold some books to the librari...      0\n",
      "4  The CEO gave the cashier cash because he wante...      1\n"
     ]
    }
   ],
   "source": [
    "#create winobias dataframe\n",
    "\n",
    "df_biased = pd.DataFrame({\"text\": biased, \"label\": 1})\n",
    "df_anti_biased = pd.DataFrame({\"text\": anti_biased, \"label\": 0})\n",
    "\n",
    "df = pd.concat([df_biased, df_anti_biased], ignore_index=True)\n",
    "\n",
    "#shuffle the dataframe\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "print(df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "class WinobiasDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].flatten(),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].flatten(),\n",
    "            \"labels\": torch.tensor(label, dtype=torch.long),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = WinobiasDataset(df[\"text\"].tolist(),df[\"label\"].tolist(),tokenizer,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_df, test_df = train_test_split(df, test_size=0.2)\n",
    "\n",
    "train_dataset = WinobiasDataset(train_df[\"text\"].tolist(),train_df[\"label\"].tolist(),tokenizer,)\n",
    "test_dataset = WinobiasDataset(test_df[\"text\"].tolist(),test_df[\"label\"].tolist(),tokenizer,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='800' max='800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [800/800 02:39, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.694754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.702511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.695537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.707431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.662932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.629631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.636200</td>\n",
       "      <td>0.746404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.636200</td>\n",
       "      <td>0.618073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.636200</td>\n",
       "      <td>0.776543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.636200</td>\n",
       "      <td>0.758271</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=800, training_loss=0.45413774967193604, metrics={'train_runtime': 160.3612, 'train_samples_per_second': 39.473, 'train_steps_per_second': 4.989, 'total_flos': 416373245107200.0, 'train_loss': 0.45413774967193604, 'epoch': 10.0})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=10,\n",
    "    eval_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unbiased\n",
      "Unbiased\n"
     ]
    }
   ],
   "source": [
    "# Predict on a new sentence\n",
    "def predict_bias(text):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=128).to(device)\n",
    "    model.to(device)\n",
    "    outputs = model(**inputs)\n",
    "    probs = torch.softmax(outputs.logits, dim=1).to(\"cpu\")\n",
    "    return \"Biased\" if torch.argmax(probs) == 1 else \"Unbiased\"\n",
    "\n",
    "print(predict_bias(\"The nurse said she would help.\"))  # Likely \"Biased\"\n",
    "print(predict_bias(\"The doctor said they would help.\"))  # Likely \"Unbiased\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
