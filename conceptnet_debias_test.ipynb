{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import statements\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from utils.ppmi import build_ppmi\n",
    "from utils.formats import load_hdf, save_hdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Test heuristics (Refactored for use with PPMI)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    \"\"\"Compute cosine similarity between two vectors.\"\"\"\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "\n",
    "def same_score(word, attribute_set, ppmi_df):\n",
    "    \"\"\"Compute the mean cosine similarity between a word and an attribute set.\"\"\"\n",
    "    if word not in ppmi_df.index:\n",
    "        return 0\n",
    "    word_vec = ppmi_df.loc[word].values\n",
    "    similarities = [\n",
    "        cosine_similarity(word_vec, ppmi_df.loc[attr].values)\n",
    "        for attr in attribute_set if attr in ppmi_df.index\n",
    "    ]\n",
    "    return np.mean(similarities) if similarities else 0\n",
    "\n",
    "def delta_same(target_set, attribute_set_a, attribute_set_b, ppmi_df):\n",
    "    \"\"\"Compute the bias score between two attribute sets using SAME.\"\"\"\n",
    "    scores_a = [same_score(word, attribute_set_a, ppmi_df) for word in target_set if word in ppmi_df.index]\n",
    "    scores_b = [same_score(word, attribute_set_b, ppmi_df) for word in target_set if word in ppmi_df.index]\n",
    "    # print(scores_a)\n",
    "    # print(scores_b)\n",
    "    return np.mean(scores_a) - np.mean(scores_b) if scores_a and scores_b else 0\n",
    "\n",
    "def compute_gender_direction(gender_pairs, ppmi_df):\n",
    "    \"\"\"Compute the gender direction based on word pairs.\"\"\"\n",
    "    differences = []\n",
    "    for male, female in gender_pairs:\n",
    "        if male in ppmi_df.index and female in ppmi_df.index:\n",
    "            diff = ppmi_df.loc[male].values - ppmi_df.loc[female].values\n",
    "            differences.append(diff)\n",
    "        else:\n",
    "            print(f\"Skipping pair ({male}, {female}) — one or both not in PPMI.\")\n",
    "    \n",
    "    if not differences:\n",
    "        raise ValueError(\"No valid gender pairs found in PPMI.\")\n",
    "\n",
    "    return np.mean(differences, axis=0)\n",
    "\n",
    "def direct_bias(word, gender_direction, ppmi_df):\n",
    "    \"\"\"Compute the direct bias of a word with respect to gender direction.\"\"\"\n",
    "    if word not in ppmi_df.index:\n",
    "        return None\n",
    "    return abs(cosine_similarity(ppmi_df.loc[word].values, gender_direction))\n",
    "\n",
    "def direct_bias_wordlist(word_list, gender_dir, ppmi_df, label, output_path):\n",
    "    results = []\n",
    "    for word in word_list:\n",
    "        if word in ppmi_df.index:\n",
    "            bias = direct_bias(word, gender_dir, ppmi_df)\n",
    "            results.append({\"word\": word, \"bias\": bias, \"group\": label})\n",
    "        else:\n",
    "            results.append({\"word\": word, \"bias\": None, \"group\": None})  # Word not in PPMI\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_csv(output_path, index=False)\n",
    "    return df\n",
    "\n",
    "def to_conceptnet_uri(word):\n",
    "    return \"/c/en/\" + word.strip().lower().replace(\" \", \"_\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Variable declarations\n",
    "ppmi_df = pd.read_hdf('data/conceptnet_api/hdf/test.hdf')\n",
    "# ppmi_df.head()\n",
    "# print(ppmi_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Reading from csv\n",
    "female_df = pd.read_csv(\"data/gendered/gender_f_cleaned.csv\", header=None)\n",
    "male_df = pd.read_csv(\"data/gendered/gender_m_cleaned.csv\", header=None)\n",
    "neutral_df = pd.read_csv(\"data/gender_neutral/gender_neutral.csv\", header=None)\n",
    "\n",
    "female_words_target = female_df[0].dropna().apply(to_conceptnet_uri).tolist()\n",
    "male_words_target = male_df[0].dropna().apply(to_conceptnet_uri).tolist()\n",
    "neutral_words_target = neutral_df[0].dropna().apply(to_conceptnet_uri).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gender direction (preview): [-5.22156241e-16 -9.12611510e-05  2.26672453e-05 -5.91807779e-06\n",
      " -9.24422121e-16]\n",
      "SAME bias score neutral: -0.0005088940741486726\n",
      "SAME bias score male: 0.26932468067546717\n",
      "SAME bias score female: -0.5780757625079354\n",
      "Direct bias for '/c/en/doctor': 0.6830964319194655\n"
     ]
    }
   ],
   "source": [
    "### Calculation of heuristics (Benchmark set as this has no edits done)\n",
    "gender_pairs = [(\"/c/en/man\", \"/c/en/woman\")]\n",
    "target_set = [\"/c/en/doctor\", \"/c/en/nurse\", \"/c/en/engineer\", \"/c/en/teacher\"]\n",
    "attribute_set_a = [\"/c/en/he\", \"/c/en/him\", \"/c/en/his\"]\n",
    "attribute_set_b = [\"/c/en/she\", \"/c/en/her\", \"/c/en/hers\"]\n",
    "\n",
    "# --- Run calculations ---\n",
    "gender_dir = compute_gender_direction(gender_pairs, ppmi_df)\n",
    "# SAME bias calculations\n",
    "bias_same_neutral = delta_same(neutral_words_target, attribute_set_a, attribute_set_b, ppmi_df)\n",
    "bias_same_male = delta_same(male_words_target, attribute_set_a, attribute_set_b, ppmi_df)\n",
    "bias_same_female = delta_same(female_words_target, attribute_set_a, attribute_set_b, ppmi_df)\n",
    "# Direct bias calculations\n",
    "bias_direct = direct_bias(\"/c/en/doctor\", gender_dir, ppmi_df)\n",
    "# Saving direct bias to csv \n",
    "output_dir = \"data/heuristic/directBias/benchmark\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "female_biases = direct_bias_wordlist(female_words_target, gender_dir, ppmi_df, \"female\", \"data/heuristic/directBias/benchmark/female_bias.csv\")\n",
    "male_biases = direct_bias_wordlist(male_words_target, gender_dir, ppmi_df_var1, \"male\", \"data/heuristic/directBias/benchmark/male_bias.csv\")\n",
    "neutral_biases = direct_bias_wordlist(neutral_words_target, gender_dir, ppmi_df_var1, \"neutral\", \"data/heuristic/directBias/benchmark/neutral_bias.csv\")\n",
    "\n",
    "print(\"Gender direction (preview):\", gender_dir[:5])\n",
    "print(\"SAME bias score neutral:\", bias_same_neutral)\n",
    "print(\"SAME bias score male:\", bias_same_male)\n",
    "print(\"SAME bias score female:\", bias_same_female)\n",
    "print(\"Direct bias for '/c/en/doctor':\", bias_direct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gender direction (preview): [-2.58369379e-15 -3.42773790e-15 -2.66589376e-05 -2.18844882e-05\n",
      " -3.24484992e-06]\n",
      "SAME bias score neutral: -0.0004291131460867348\n",
      "SAME bias score male: 0.2689158195800157\n",
      "SAME bias score female: -0.5813703747456963\n",
      "Direct bias for '/c/en/doctor': 0.6820667020592693\n"
     ]
    }
   ],
   "source": [
    "#### Code above was functionality testing, code below demonstrates actual work tried\n",
    "# Variation 1: Small dataset + Filter (Antonyms)\n",
    "# First have to edit scraper for filtering.\n",
    "df = pd.read_csv(\"data/conceptnet_api/csv/edge_extractVar1.csv\")\n",
    "# print(df.shape)\n",
    "# print(df['weight'].describe())\n",
    "# df.head(3)\n",
    "ppmi_df_var1 = build_ppmi(conceptnet_filename=\"data/conceptnet_api/csv/edge_extractVar1.csv\", ndim=300)\n",
    "save_hdf(ppmi_df_var1, filename='data/conceptnet_api/hdf/testVar1.hdf')\n",
    "\n",
    "gender_pairs = [(\"/c/en/man\", \"/c/en/woman\")]\n",
    "attribute_set_a = [\"/c/en/he\", \"/c/en/him\", \"/c/en/his\"]\n",
    "attribute_set_b = [\"/c/en/she\", \"/c/en/her\", \"/c/en/hers\"]\n",
    "\n",
    "# --- Run calculations ---\n",
    "gender_dir = compute_gender_direction(gender_pairs, ppmi_df_var1)\n",
    "# SAME bias calculations\n",
    "bias_same_neutral = delta_same(neutral_words_target, attribute_set_a, attribute_set_b, ppmi_df_var1)\n",
    "bias_same_male = delta_same(male_words_target, attribute_set_a, attribute_set_b, ppmi_df_var1)\n",
    "bias_same_female = delta_same(female_words_target, attribute_set_a, attribute_set_b, ppmi_df_var1)\n",
    "# Direct bias calculations\n",
    "bias_direct = direct_bias(\"/c/en/doctor\", gender_dir, ppmi_df_var1)\n",
    "# Saving direct bias to csv \n",
    "output_dir = \"data/heuristic/directBias/variation1\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "female_biases = direct_bias_wordlist(female_words_target, gender_dir, ppmi_df_var1, \"female\", \"data/heuristic/directBias/variation1/female_bias.csv\")\n",
    "male_biases = direct_bias_wordlist(male_words_target, gender_dir, ppmi_df_var1, \"male\", \"data/heuristic/directBias/variation1/male_bias.csv\")\n",
    "neutral_biases = direct_bias_wordlist(neutral_words_target, gender_dir, ppmi_df_var1, \"neutral\", \"data/heuristic/directBias/variation1/neutral_bias.csv\")\n",
    "\n",
    "print(\"Gender direction (preview):\", gender_dir[:5])\n",
    "print(\"SAME bias score neutral:\", bias_same_neutral)\n",
    "print(\"SAME bias score male:\", bias_same_male)\n",
    "print(\"SAME bias score female:\", bias_same_female)\n",
    "print(\"Direct bias for '/c/en/doctor':\", bias_direct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gender direction (preview): [-3.58110229e-16 -4.51613485e-15  1.84086028e-15  4.83755087e-15\n",
      "  3.60902266e-15]\n",
      "SAME bias score neutral: -0.0005454765421955589\n",
      "SAME bias score male: 0.26817594971248154\n",
      "SAME bias score female: -0.5816567944429107\n",
      "Direct bias for '/c/en/doctor': 0.6483519823562611\n"
     ]
    }
   ],
   "source": [
    "# Variation 2: Small dataset + Filter (\"/r/Antonym\", \"/r/NotDesires\", \"/r/Desires\", \"/r/ObstructedBy\", \"/r/MannerOf\")\n",
    "# First have to edit scraper for filtering.\n",
    "df = pd.read_csv(\"data/conceptnet_api/csv/edge_extractVar2.csv\")\n",
    "# print(df.shape)\n",
    "# print(df['weight'].describe())\n",
    "# df.head(3)\n",
    "ppmi_df_var2 = build_ppmi(conceptnet_filename=\"data/conceptnet_api/csv/edge_extractVar2.csv\", ndim=300)\n",
    "save_hdf(ppmi_df_var2, filename='data/conceptnet_api/hdf/testVar2.hdf')\n",
    "\n",
    "gender_pairs = [(\"/c/en/man\", \"/c/en/woman\")]\n",
    "attribute_set_a = [\"/c/en/he\", \"/c/en/him\", \"/c/en/his\"]\n",
    "attribute_set_b = [\"/c/en/she\", \"/c/en/her\", \"/c/en/hers\"]\n",
    "\n",
    "# --- Run calculations ---\n",
    "gender_dir = compute_gender_direction(gender_pairs, ppmi_df_var2)\n",
    "# SAME bias calculations\n",
    "bias_same_neutral = delta_same(neutral_words_target, attribute_set_a, attribute_set_b, ppmi_df_var2)\n",
    "bias_same_male = delta_same(male_words_target, attribute_set_a, attribute_set_b, ppmi_df_var2)\n",
    "bias_same_female = delta_same(female_words_target, attribute_set_a, attribute_set_b, ppmi_df_var2)\n",
    "# Direct bias calculations\n",
    "bias_direct = direct_bias(\"/c/en/doctor\", gender_dir, ppmi_df_var2)\n",
    "# Saving direct bias to csv \n",
    "output_dir = \"data/heuristic/directBias/variation2\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "female_biases = direct_bias_wordlist(female_words_target, gender_dir, ppmi_df_var2, \"female\", \"data/heuristic/directBias/variation2/female_bias.csv\")\n",
    "male_biases = direct_bias_wordlist(male_words_target, gender_dir, ppmi_df_var2, \"male\", \"data/heuristic/directBias/variation2/male_bias.csv\")\n",
    "neutral_biases = direct_bias_wordlist(neutral_words_target, gender_dir, ppmi_df_var2, \"neutral\", \"data/heuristic/directBias/variation2/neutral_bias.csv\")\n",
    "\n",
    "print(\"Gender direction (preview):\", gender_dir[:5])\n",
    "print(\"SAME bias score neutral:\", bias_same_neutral)\n",
    "print(\"SAME bias score male:\", bias_same_male)\n",
    "print(\"SAME bias score female:\", bias_same_female)\n",
    "print(\"Direct bias for '/c/en/doctor':\", bias_direct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neutral coverage:\n",
      "✔ Found 57 words, ❌ Missing 17 words\n",
      "Male coverage:\n",
      "✔ Found 29 words, ❌ Missing 494 words\n",
      "Female coverage:\n",
      "✔ Found 15 words, ❌ Missing 352 words\n",
      "Index(['/c/en/help_child', '/c/en/adult', '/c/en/man', '/c/en/sign_contract',\n",
      "       '/c/en/dress_herself', '/c/en/sheep', '/c/en/adult/n/wn/person',\n",
      "       '/c/en/fascist/n/wn/person', '/c/en/man/n/wn/person',\n",
      "       '/c/en/stay_at_home/n/wn/person',\n",
      "       ...\n",
      "       '/c/en/quarryman/n/wn/person', '/c/en/slave/n/wn/person',\n",
      "       '/c/en/tier/n/wn/person', '/c/en/political_officer/n',\n",
      "       '/c/en/employable/n/wn/person', '/c/en/throwster/n/wn/person',\n",
      "       '/c/en/freelance/n/wn/person', '/c/en/skidder/n/wn/person',\n",
      "       '/c/en/solderer/n/wn/person', '/c/en/bleacher/n/wn/person'],\n",
      "      dtype='object', length=3958)\n"
     ]
    }
   ],
   "source": [
    "def check_vocab_coverage(word_list, ppmi_df):\n",
    "    present = [w for w in word_list if w in ppmi_df.index]\n",
    "    missing = [w for w in word_list if w not in ppmi_df.index]\n",
    "    print(f\"✔ Found {len(present)} words, ❌ Missing {len(missing)} words\")\n",
    "    return present, missing\n",
    "\n",
    "print(\"Neutral coverage:\")\n",
    "_, _ = check_vocab_coverage(neutral_words_target, ppmi_df_var2)\n",
    "\n",
    "print(\"Male coverage:\")\n",
    "_, _ = check_vocab_coverage(male_words_target, ppmi_df_var2)\n",
    "\n",
    "print(\"Female coverage:\")\n",
    "_, _ = check_vocab_coverage(female_words_target, ppmi_df_var2)\n",
    "\n",
    "print(ppmi_df_var2.index)\n",
    "\n",
    "## From here, realised that we are not getting alot of coverage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gender direction (preview): [-1.48005486e-15  1.40020227e-14  8.57734504e-16  4.60818214e-15\n",
      "  7.30527111e-15]\n",
      "SAME bias score neutral: 0.00016561382623995688\n",
      "SAME bias score male: 0.2811464336126861\n",
      "SAME bias score female: -0.5822122091100196\n",
      "Direct bias for '/c/en/doctor': 0.6898718438347983\n"
     ]
    }
   ],
   "source": [
    "# Variation 3: Small dataset + Filter (Unidirectional edges)\n",
    "# First have to edit scraper for filtering.\n",
    "df = pd.read_csv(\"data/conceptnet_api/csv/edge_extractVar3.csv\")\n",
    "# print(df.shape)\n",
    "# print(df['weight'].describe())\n",
    "# df.head(3)\n",
    "# Edit the Var numbers below. E.g. edge_extractVar<NUMBER> and testVar<NUMBER>\n",
    "ppmi_df_var3 = build_ppmi(conceptnet_filename=\"data/conceptnet_api/csv/edge_extractVar3.csv\", ndim=300)\n",
    "save_hdf(ppmi_df_var3, filename='data/conceptnet_api/hdf/testVar3.hdf')\n",
    "\n",
    "gender_pairs = [(\"/c/en/man\", \"/c/en/woman\")]\n",
    "attribute_set_a = [\"/c/en/he\", \"/c/en/him\", \"/c/en/his\"]\n",
    "attribute_set_b = [\"/c/en/she\", \"/c/en/her\", \"/c/en/hers\"]\n",
    "\n",
    "# --- Run calculations ---\n",
    "gender_dir = compute_gender_direction(gender_pairs, ppmi_df_var3)\n",
    "# SAME bias calculations\n",
    "bias_same_neutral = delta_same(neutral_words_target, attribute_set_a, attribute_set_b, ppmi_df_var3)\n",
    "bias_same_male = delta_same(male_words_target, attribute_set_a, attribute_set_b, ppmi_df_var3)\n",
    "bias_same_female = delta_same(female_words_target, attribute_set_a, attribute_set_b, ppmi_df_var3)\n",
    "# Direct bias calculations\n",
    "bias_direct = direct_bias(\"/c/en/doctor\", gender_dir, ppmi_df_var3)\n",
    "# Saving direct bias to csv \n",
    "output_dir = \"data/heuristic/directBias/variation3\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "female_biases = direct_bias_wordlist(female_words_target, gender_dir, ppmi_df_var3, \"female\", \"data/heuristic/directBias/variation3/female_bias.csv\")\n",
    "male_biases = direct_bias_wordlist(male_words_target, gender_dir, ppmi_df_var3, \"male\", \"data/heuristic/directBias/variation3/male_bias.csv\")\n",
    "neutral_biases = direct_bias_wordlist(neutral_words_target, gender_dir, ppmi_df_var3, \"neutral\", \"data/heuristic/directBias/variation3/neutral_bias.csv\")\n",
    "\n",
    "print(\"Gender direction (preview):\", gender_dir[:5])\n",
    "print(\"SAME bias score neutral:\", bias_same_neutral)\n",
    "print(\"SAME bias score male:\", bias_same_male)\n",
    "print(\"SAME bias score female:\", bias_same_female)\n",
    "print(\"Direct bias for '/c/en/doctor':\", bias_direct)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CS4248",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
